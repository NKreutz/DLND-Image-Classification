{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = '/input/cifar-10/python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 0 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7631a80cf8>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    return np.array(x/255.0)\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return np.eye(10)[x]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a bach of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, shape = (None, image_shape[0],image_shape[1],image_shape[2]),name = 'x')\n",
    "   \n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, shape = (None, n_classes), name = 'y')\n",
    "    \n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, name = 'keep_prob') \n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow placeholder Tensor #shape = batch size(None(any)), 32x32 (height and width), depth of 5\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer = 10 # depth of the filter\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer = (2,2) # patch height and width in a tuple\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution = (4,4)\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool = (2,2) # patch size for max pooling layer\n",
    "    :param pool_strides: Stride 2-D Tuple for pool = (2,2)\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    # Create the weight and bias using conv_ksize, conv_num_outputs and the shape of x_tensor.\n",
    "    filter_weights = tf.Variable(tf.truncated_normal([conv_ksize[0], conv_ksize[1], x_tensor.get_shape().as_list()[3], conv_num_outputs], stddev=0.1))\n",
    "    filter_bias = tf.Variable(tf.zeros(conv_num_outputs))\n",
    "    \n",
    "    # Apply a convolution to x_tensor using weight and conv_strides.\n",
    "    strides = [1, conv_strides[0], conv_strides[1], 1] # (batch, height, width, depth)\n",
    "    conv_layer = tf.nn.conv2d(x_tensor, filter_weights, strides, padding='SAME')\n",
    "    \n",
    "    #Add bias\n",
    "    conv_layer = tf.nn.bias_add(conv_layer, filter_bias)\n",
    "    \n",
    "    #Add a nonlinear activation to the convolution\n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "\n",
    "    # Apply Max Pooling using pool_ksize and pool_strides\n",
    "    strides = [1, pool_strides[0], pool_strides[1], 1]\n",
    "    pool_ksize = [1, pool_ksize[0], pool_ksize[1], 1]\n",
    "    conv_layer = tf.nn.max_pool(conv_layer, pool_ksize, strides, padding='SAME')\n",
    "    \n",
    "    return conv_layer\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    height = x_tensor.get_shape().as_list()[1]\n",
    "    width = x_tensor.get_shape().as_list()[2]\n",
    "    depth = x_tensor.get_shape().as_list()[3]\n",
    "    flat_image_size = height * width * depth\n",
    "    return tf.reshape(x_tensor,(-1, flat_image_size))\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size. =(?, 128)\n",
    "    : num_outputs: The number of output that the new tensor should be. = 40\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "\n",
    "    weights = tf.Variable(tf.truncated_normal((x_tensor.get_shape().as_list()[1], num_outputs), stddev = 0.1))\n",
    "    bias = tf.Variable(tf.zeros(num_outputs))\n",
    "    \n",
    "    fully_conn_layer = tf.add(tf.matmul(x_tensor, weights), bias)\n",
    "    return tf.nn.relu(fully_conn_layer)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    weights = tf.Variable(tf.truncated_normal((x_tensor.get_shape().as_list()[1], num_outputs), stddev = 0.1))\n",
    "    bias = tf.Variable(tf.zeros(num_outputs))\n",
    "    \n",
    "   \n",
    "    return tf.add(tf.matmul(x_tensor, weights), bias)\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    conv_layer_1 = conv2d_maxpool(x, 8, (5,5),(2,2),(2,2),(2,2))\n",
    "    conv_layer_2 = conv2d_maxpool(conv_layer_1, 15, (3,3), (1,1),(2,2), (2,2))\n",
    "    \n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    flat = flatten(conv_layer_2)\n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    fc_layer_1 = fully_conn(flat, 40)\n",
    "    fc_layer_1 = tf.nn.dropout(fc_layer_1, keep_prob)\n",
    "    fc_layer_2 = fully_conn(fc_layer_1, 20)\n",
    "    fc_layer_2 = tf.nn.dropout(fc_layer_1, keep_prob)\n",
    "    fc_layer_3 = fully_conn(fc_layer_1, 15)\n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    logits = output(fc_layer_2, 10)\n",
    "    \n",
    "    # TODO: return output\n",
    "    return logits\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    session.run(optimizer, feed_dict = {x:feature_batch, y:label_batch, keep_prob:keep_probability})\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    train_accuracy = session.run(accuracy, feed_dict={x: feature_batch, y: label_batch, keep_prob: 1.0})\n",
    "    train_cost = session.run(cost, feed_dict={x: feature_batch, y: label_batch, keep_prob: 1.0})\n",
    "    validation_accuracy = session.run(accuracy, feed_dict={x: valid_features, y: valid_labels, keep_prob: 1.0})\n",
    "    validation_cost = session.run(cost, feed_dict={x: valid_features, y: valid_labels, keep_prob: 1.0})\n",
    "\n",
    "    \n",
    "    print('Accuracy: Training: {} Validation: {} '.format(train_accuracy, validation_accuracy, ))\n",
    "    print('Loss: Training: {} Validation: {}'.format(train_cost,  validation_cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 90\n",
    "batch_size = 128\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Accuracy: Training: 0.17499999701976776 Validation: 0.19300000369548798 \n",
      "Loss: Training: 2.274491310119629 Validation: 2.2739205360412598\n",
      "Epoch  2, CIFAR-10 Batch 1:  Accuracy: Training: 0.17499999701976776 Validation: 0.24619996547698975 \n",
      "Loss: Training: 2.219951868057251 Validation: 2.1531050205230713\n",
      "Epoch  3, CIFAR-10 Batch 1:  Accuracy: Training: 0.17499999701976776 Validation: 0.2937999665737152 \n",
      "Loss: Training: 2.1440775394439697 Validation: 2.005606174468994\n",
      "Epoch  4, CIFAR-10 Batch 1:  Accuracy: Training: 0.22500000894069672 Validation: 0.32339999079704285 \n",
      "Loss: Training: 2.1464579105377197 Validation: 1.92202889919281\n",
      "Epoch  5, CIFAR-10 Batch 1:  Accuracy: Training: 0.2750000059604645 Validation: 0.33079999685287476 \n",
      "Loss: Training: 2.135658025741577 Validation: 1.8710564374923706\n",
      "Epoch  6, CIFAR-10 Batch 1:  Accuracy: Training: 0.2500000298023224 Validation: 0.3410000205039978 \n",
      "Loss: Training: 2.111262798309326 Validation: 1.8704088926315308\n",
      "Epoch  7, CIFAR-10 Batch 1:  Accuracy: Training: 0.30000001192092896 Validation: 0.3610000014305115 \n",
      "Loss: Training: 2.1214230060577393 Validation: 1.8378633260726929\n",
      "Epoch  8, CIFAR-10 Batch 1:  Accuracy: Training: 0.2750000059604645 Validation: 0.3585999608039856 \n",
      "Loss: Training: 2.085170269012451 Validation: 1.8189787864685059\n",
      "Epoch  9, CIFAR-10 Batch 1:  Accuracy: Training: 0.3500000238418579 Validation: 0.3725999593734741 \n",
      "Loss: Training: 2.0982584953308105 Validation: 1.7889715433120728\n",
      "Epoch 10, CIFAR-10 Batch 1:  Accuracy: Training: 0.32499998807907104 Validation: 0.36959999799728394 \n",
      "Loss: Training: 2.0951504707336426 Validation: 1.7978870868682861\n",
      "Epoch 11, CIFAR-10 Batch 1:  Accuracy: Training: 0.30000001192092896 Validation: 0.3686000108718872 \n",
      "Loss: Training: 2.0811705589294434 Validation: 1.780035376548767\n",
      "Epoch 12, CIFAR-10 Batch 1:  Accuracy: Training: 0.3500000238418579 Validation: 0.37699997425079346 \n",
      "Loss: Training: 2.0686960220336914 Validation: 1.7648593187332153\n",
      "Epoch 13, CIFAR-10 Batch 1:  Accuracy: Training: 0.22499999403953552 Validation: 0.33739998936653137 \n",
      "Loss: Training: 2.1542696952819824 Validation: 1.863511562347412\n",
      "Epoch 14, CIFAR-10 Batch 1:  Accuracy: Training: 0.3500000238418579 Validation: 0.37779998779296875 \n",
      "Loss: Training: 2.070639133453369 Validation: 1.7577543258666992\n",
      "Epoch 15, CIFAR-10 Batch 1:  Accuracy: Training: 0.32500001788139343 Validation: 0.3765999972820282 \n",
      "Loss: Training: 2.0165090560913086 Validation: 1.7373374700546265\n",
      "Epoch 16, CIFAR-10 Batch 1:  Accuracy: Training: 0.3500000238418579 Validation: 0.3871999979019165 \n",
      "Loss: Training: 2.049805164337158 Validation: 1.7341805696487427\n",
      "Epoch 17, CIFAR-10 Batch 1:  Accuracy: Training: 0.32500001788139343 Validation: 0.3846000134944916 \n",
      "Loss: Training: 2.037038803100586 Validation: 1.7195183038711548\n",
      "Epoch 18, CIFAR-10 Batch 1:  Accuracy: Training: 0.375 Validation: 0.3937999904155731 \n",
      "Loss: Training: 2.0214388370513916 Validation: 1.697009801864624\n",
      "Epoch 19, CIFAR-10 Batch 1:  Accuracy: Training: 0.30000001192092896 Validation: 0.39159995317459106 \n",
      "Loss: Training: 2.0328030586242676 Validation: 1.7395198345184326\n",
      "Epoch 20, CIFAR-10 Batch 1:  Accuracy: Training: 0.2750000059604645 Validation: 0.3829999566078186 \n",
      "Loss: Training: 1.9990575313568115 Validation: 1.7037227153778076\n",
      "Epoch 21, CIFAR-10 Batch 1:  Accuracy: Training: 0.3500000238418579 Validation: 0.3943999707698822 \n",
      "Loss: Training: 1.9712157249450684 Validation: 1.6873540878295898\n",
      "Epoch 22, CIFAR-10 Batch 1:  Accuracy: Training: 0.42500001192092896 Validation: 0.4034000039100647 \n",
      "Loss: Training: 1.9978067874908447 Validation: 1.6738712787628174\n",
      "Epoch 23, CIFAR-10 Batch 1:  Accuracy: Training: 0.3500000238418579 Validation: 0.40939998626708984 \n",
      "Loss: Training: 1.9633991718292236 Validation: 1.6722053289413452\n",
      "Epoch 24, CIFAR-10 Batch 1:  Accuracy: Training: 0.42499998211860657 Validation: 0.38759997487068176 \n",
      "Loss: Training: 1.991816759109497 Validation: 1.7006580829620361\n",
      "Epoch 25, CIFAR-10 Batch 1:  Accuracy: Training: 0.375 Validation: 0.39899998903274536 \n",
      "Loss: Training: 2.010530948638916 Validation: 1.6816506385803223\n",
      "Epoch 26, CIFAR-10 Batch 1:  Accuracy: Training: 0.40000003576278687 Validation: 0.41019999980926514 \n",
      "Loss: Training: 1.9412498474121094 Validation: 1.646310806274414\n",
      "Epoch 27, CIFAR-10 Batch 1:  Accuracy: Training: 0.3750000298023224 Validation: 0.41279998421669006 \n",
      "Loss: Training: 1.9439138174057007 Validation: 1.645085096359253\n",
      "Epoch 28, CIFAR-10 Batch 1:  Accuracy: Training: 0.42500001192092896 Validation: 0.4205999970436096 \n",
      "Loss: Training: 1.9369440078735352 Validation: 1.6428964138031006\n",
      "Epoch 29, CIFAR-10 Batch 1:  Accuracy: Training: 0.42500001192092896 Validation: 0.41359999775886536 \n",
      "Loss: Training: 1.9079240560531616 Validation: 1.6256749629974365\n",
      "Epoch 30, CIFAR-10 Batch 1:  Accuracy: Training: 0.4750000238418579 Validation: 0.4235999882221222 \n",
      "Loss: Training: 1.8720687627792358 Validation: 1.634120225906372\n",
      "Epoch 31, CIFAR-10 Batch 1:  Accuracy: Training: 0.42500001192092896 Validation: 0.4095999598503113 \n",
      "Loss: Training: 1.9059370756149292 Validation: 1.647468090057373\n",
      "Epoch 32, CIFAR-10 Batch 1:  Accuracy: Training: 0.5 Validation: 0.4095999598503113 \n",
      "Loss: Training: 1.8427808284759521 Validation: 1.6302697658538818\n",
      "Epoch 33, CIFAR-10 Batch 1:  Accuracy: Training: 0.45000001788139343 Validation: 0.4283999800682068 \n",
      "Loss: Training: 1.8479009866714478 Validation: 1.6186151504516602\n",
      "Epoch 34, CIFAR-10 Batch 1:  Accuracy: Training: 0.45000001788139343 Validation: 0.4331999719142914 \n",
      "Loss: Training: 1.8679661750793457 Validation: 1.6085741519927979\n",
      "Epoch 35, CIFAR-10 Batch 1:  Accuracy: Training: 0.45000001788139343 Validation: 0.4139999747276306 \n",
      "Loss: Training: 1.8502297401428223 Validation: 1.63957941532135\n",
      "Epoch 36, CIFAR-10 Batch 1:  Accuracy: Training: 0.45000001788139343 Validation: 0.4301999807357788 \n",
      "Loss: Training: 1.795208215713501 Validation: 1.6112905740737915\n",
      "Epoch 37, CIFAR-10 Batch 1:  Accuracy: Training: 0.4749999940395355 Validation: 0.42639997601509094 \n",
      "Loss: Training: 1.7968225479125977 Validation: 1.6150381565093994\n",
      "Epoch 38, CIFAR-10 Batch 1:  Accuracy: Training: 0.550000011920929 Validation: 0.43539994955062866 \n",
      "Loss: Training: 1.741228461265564 Validation: 1.596514344215393\n",
      "Epoch 39, CIFAR-10 Batch 1:  Accuracy: Training: 0.5250000357627869 Validation: 0.41999995708465576 \n",
      "Loss: Training: 1.7699997425079346 Validation: 1.6206345558166504\n",
      "Epoch 40, CIFAR-10 Batch 1:  Accuracy: Training: 0.5 Validation: 0.44099995493888855 \n",
      "Loss: Training: 1.7522478103637695 Validation: 1.5824679136276245\n",
      "Epoch 41, CIFAR-10 Batch 1:  Accuracy: Training: 0.5 Validation: 0.43939998745918274 \n",
      "Loss: Training: 1.7236179113388062 Validation: 1.5902081727981567\n",
      "Epoch 42, CIFAR-10 Batch 1:  Accuracy: Training: 0.5250000357627869 Validation: 0.4371999502182007 \n",
      "Loss: Training: 1.706434726715088 Validation: 1.5858829021453857\n",
      "Epoch 43, CIFAR-10 Batch 1:  Accuracy: Training: 0.5250000357627869 Validation: 0.43479999899864197 \n",
      "Loss: Training: 1.709486484527588 Validation: 1.5937526226043701\n",
      "Epoch 44, CIFAR-10 Batch 1:  Accuracy: Training: 0.6000000238418579 Validation: 0.437999963760376 \n",
      "Loss: Training: 1.6786980628967285 Validation: 1.5734435319900513\n",
      "Epoch 45, CIFAR-10 Batch 1:  Accuracy: Training: 0.5750000476837158 Validation: 0.4283999800682068 \n",
      "Loss: Training: 1.6513526439666748 Validation: 1.5893086194992065\n",
      "Epoch 46, CIFAR-10 Batch 1:  Accuracy: Training: 0.45000001788139343 Validation: 0.44179999828338623 \n",
      "Loss: Training: 1.6952283382415771 Validation: 1.5806758403778076\n",
      "Epoch 47, CIFAR-10 Batch 1:  Accuracy: Training: 0.5750000476837158 Validation: 0.43140000104904175 \n",
      "Loss: Training: 1.6116933822631836 Validation: 1.5710556507110596\n",
      "Epoch 48, CIFAR-10 Batch 1:  Accuracy: Training: 0.5750000476837158 Validation: 0.4403999447822571 \n",
      "Loss: Training: 1.7015223503112793 Validation: 1.577215552330017\n",
      "Epoch 49, CIFAR-10 Batch 1:  Accuracy: Training: 0.5250000357627869 Validation: 0.4429999589920044 \n",
      "Loss: Training: 1.660507321357727 Validation: 1.5624141693115234\n",
      "Epoch 50, CIFAR-10 Batch 1:  Accuracy: Training: 0.6000000238418579 Validation: 0.4365999698638916 \n",
      "Loss: Training: 1.610716700553894 Validation: 1.5648603439331055\n",
      "Epoch 51, CIFAR-10 Batch 1:  Accuracy: Training: 0.6000000238418579 Validation: 0.44439998269081116 \n",
      "Loss: Training: 1.6120948791503906 Validation: 1.552267074584961\n",
      "Epoch 52, CIFAR-10 Batch 1:  Accuracy: Training: 0.5250000357627869 Validation: 0.44999998807907104 \n",
      "Loss: Training: 1.6089839935302734 Validation: 1.5517542362213135\n",
      "Epoch 53, CIFAR-10 Batch 1:  Accuracy: Training: 0.5750000476837158 Validation: 0.44439998269081116 \n",
      "Loss: Training: 1.5778266191482544 Validation: 1.5466206073760986\n",
      "Epoch 54, CIFAR-10 Batch 1:  Accuracy: Training: 0.6000000238418579 Validation: 0.4439999759197235 \n",
      "Loss: Training: 1.535657525062561 Validation: 1.542886734008789\n",
      "Epoch 55, CIFAR-10 Batch 1:  Accuracy: Training: 0.5750000476837158 Validation: 0.44279995560646057 \n",
      "Loss: Training: 1.5498228073120117 Validation: 1.5530534982681274\n",
      "Epoch 56, CIFAR-10 Batch 1:  Accuracy: Training: 0.5750000476837158 Validation: 0.44579997658729553 \n",
      "Loss: Training: 1.5585417747497559 Validation: 1.547319769859314\n",
      "Epoch 57, CIFAR-10 Batch 1:  Accuracy: Training: 0.6000000238418579 Validation: 0.44019997119903564 \n",
      "Loss: Training: 1.5834883451461792 Validation: 1.556707739830017\n",
      "Epoch 58, CIFAR-10 Batch 1:  Accuracy: Training: 0.675000011920929 Validation: 0.44099998474121094 \n",
      "Loss: Training: 1.5206964015960693 Validation: 1.5466364622116089\n",
      "Epoch 59, CIFAR-10 Batch 1:  Accuracy: Training: 0.6000000238418579 Validation: 0.43759995698928833 \n",
      "Loss: Training: 1.5732108354568481 Validation: 1.550780177116394\n",
      "Epoch 60, CIFAR-10 Batch 1:  Accuracy: Training: 0.5750000476837158 Validation: 0.4439999461174011 \n",
      "Loss: Training: 1.5683070421218872 Validation: 1.5322474241256714\n",
      "Epoch 61, CIFAR-10 Batch 1:  Accuracy: Training: 0.6000000238418579 Validation: 0.449599951505661 \n",
      "Loss: Training: 1.5373296737670898 Validation: 1.538374423980713\n",
      "Epoch 62, CIFAR-10 Batch 1:  Accuracy: Training: 0.5750000476837158 Validation: 0.4311999976634979 \n",
      "Loss: Training: 1.5988647937774658 Validation: 1.5706595182418823\n",
      "Epoch 63, CIFAR-10 Batch 1:  Accuracy: Training: 0.7000000476837158 Validation: 0.44759994745254517 \n",
      "Loss: Training: 1.4794921875 Validation: 1.5270631313323975\n",
      "Epoch 64, CIFAR-10 Batch 1:  Accuracy: Training: 0.675000011920929 Validation: 0.44919997453689575 \n",
      "Loss: Training: 1.502467393875122 Validation: 1.532867193222046\n",
      "Epoch 65, CIFAR-10 Batch 1:  Accuracy: Training: 0.675000011920929 Validation: 0.4577999711036682 \n",
      "Loss: Training: 1.4739729166030884 Validation: 1.5404083728790283\n",
      "Epoch 66, CIFAR-10 Batch 1:  Accuracy: Training: 0.6750000715255737 Validation: 0.44979995489120483 \n",
      "Loss: Training: 1.457608938217163 Validation: 1.5223338603973389\n",
      "Epoch 67, CIFAR-10 Batch 1:  Accuracy: Training: 0.6500000357627869 Validation: 0.4487999379634857 \n",
      "Loss: Training: 1.4382866621017456 Validation: 1.5185573101043701\n",
      "Epoch 68, CIFAR-10 Batch 1:  Accuracy: Training: 0.6000000238418579 Validation: 0.45159995555877686 \n",
      "Loss: Training: 1.4898045063018799 Validation: 1.522863507270813\n",
      "Epoch 69, CIFAR-10 Batch 1:  Accuracy: Training: 0.6500000357627869 Validation: 0.45899999141693115 \n",
      "Loss: Training: 1.4609272480010986 Validation: 1.5154393911361694\n",
      "Epoch 70, CIFAR-10 Batch 1:  Accuracy: Training: 0.550000011920929 Validation: 0.4449999928474426 \n",
      "Loss: Training: 1.4943004846572876 Validation: 1.5405075550079346\n",
      "Epoch 71, CIFAR-10 Batch 1:  Accuracy: Training: 0.574999988079071 Validation: 0.4517999589443207 \n",
      "Loss: Training: 1.4692100286483765 Validation: 1.5221452713012695\n",
      "Epoch 72, CIFAR-10 Batch 1:  Accuracy: Training: 0.675000011920929 Validation: 0.45139995217323303 \n",
      "Loss: Training: 1.4408202171325684 Validation: 1.520871877670288\n",
      "Epoch 73, CIFAR-10 Batch 1:  Accuracy: Training: 0.6500000357627869 Validation: 0.44999998807907104 \n",
      "Loss: Training: 1.4335072040557861 Validation: 1.5101065635681152\n",
      "Epoch 74, CIFAR-10 Batch 1:  Accuracy: Training: 0.6500000357627869 Validation: 0.45339998602867126 \n",
      "Loss: Training: 1.414375901222229 Validation: 1.516091227531433\n",
      "Epoch 75, CIFAR-10 Batch 1:  Accuracy: Training: 0.6250000596046448 Validation: 0.4487999677658081 \n",
      "Loss: Training: 1.4386552572250366 Validation: 1.512844204902649\n",
      "Epoch 76, CIFAR-10 Batch 1:  Accuracy: Training: 0.6000000238418579 Validation: 0.4665999412536621 \n",
      "Loss: Training: 1.4234869480133057 Validation: 1.5052902698516846\n",
      "Epoch 77, CIFAR-10 Batch 1:  Accuracy: Training: 0.5750000476837158 Validation: 0.4405999779701233 \n",
      "Loss: Training: 1.5131118297576904 Validation: 1.5480074882507324\n",
      "Epoch 78, CIFAR-10 Batch 1:  Accuracy: Training: 0.6500000357627869 Validation: 0.46039995551109314 \n",
      "Loss: Training: 1.419365406036377 Validation: 1.5034557580947876\n",
      "Epoch 79, CIFAR-10 Batch 1:  Accuracy: Training: 0.6500000357627869 Validation: 0.4535999894142151 \n",
      "Loss: Training: 1.4001898765563965 Validation: 1.5001897811889648\n",
      "Epoch 80, CIFAR-10 Batch 1:  Accuracy: Training: 0.6000000238418579 Validation: 0.45559996366500854 \n",
      "Loss: Training: 1.416768193244934 Validation: 1.507910966873169\n",
      "Epoch 81, CIFAR-10 Batch 1:  Accuracy: Training: 0.625 Validation: 0.4537999629974365 \n",
      "Loss: Training: 1.375483512878418 Validation: 1.4992893934249878\n",
      "Epoch 82, CIFAR-10 Batch 1:  Accuracy: Training: 0.625 Validation: 0.4593999981880188 \n",
      "Loss: Training: 1.360790491104126 Validation: 1.4982861280441284\n",
      "Epoch 83, CIFAR-10 Batch 1:  Accuracy: Training: 0.625 Validation: 0.4559999704360962 \n",
      "Loss: Training: 1.3824899196624756 Validation: 1.5016201734542847\n",
      "Epoch 84, CIFAR-10 Batch 1:  Accuracy: Training: 0.7000000476837158 Validation: 0.46539995074272156 \n",
      "Loss: Training: 1.3439314365386963 Validation: 1.485939621925354\n",
      "Epoch 85, CIFAR-10 Batch 1:  Accuracy: Training: 0.7000000476837158 Validation: 0.4699999690055847 \n",
      "Loss: Training: 1.361148476600647 Validation: 1.484022617340088\n",
      "Epoch 86, CIFAR-10 Batch 1:  Accuracy: Training: 0.6750000715255737 Validation: 0.4723999500274658 \n",
      "Loss: Training: 1.3364509344100952 Validation: 1.4841418266296387\n",
      "Epoch 87, CIFAR-10 Batch 1:  Accuracy: Training: 0.7000000476837158 Validation: 0.47019994258880615 \n",
      "Loss: Training: 1.3303556442260742 Validation: 1.479522943496704\n",
      "Epoch 88, CIFAR-10 Batch 1:  Accuracy: Training: 0.7000000476837158 Validation: 0.4761999845504761 \n",
      "Loss: Training: 1.3196625709533691 Validation: 1.4697415828704834\n",
      "Epoch 89, CIFAR-10 Batch 1:  Accuracy: Training: 0.675000011920929 Validation: 0.4723999500274658 \n",
      "Loss: Training: 1.3335312604904175 Validation: 1.4897987842559814\n",
      "Epoch 90, CIFAR-10 Batch 1:  Accuracy: Training: 0.7000000476837158 Validation: 0.46599993109703064 \n",
      "Loss: Training: 1.329162836074829 Validation: 1.49077308177948\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Accuracy: Training: 0.17500001192092896 Validation: 0.1761999875307083 \n",
      "Loss: Training: 2.2751307487487793 Validation: 2.2625460624694824\n",
      "Epoch  1, CIFAR-10 Batch 2:  Accuracy: Training: 0.32499998807907104 Validation: 0.2975999712944031 \n",
      "Loss: Training: 2.076916456222534 Validation: 2.0588839054107666\n",
      "Epoch  1, CIFAR-10 Batch 3:  Accuracy: Training: 0.3499999940395355 Validation: 0.31919997930526733 \n",
      "Loss: Training: 1.8780566453933716 Validation: 1.9285154342651367\n",
      "Epoch  1, CIFAR-10 Batch 4:  Accuracy: Training: 0.3500000238418579 Validation: 0.3691999614238739 \n",
      "Loss: Training: 1.8729498386383057 Validation: 1.8550124168395996\n",
      "Epoch  1, CIFAR-10 Batch 5:  Accuracy: Training: 0.3500000238418579 Validation: 0.3773999810218811 \n",
      "Loss: Training: 1.8649967908859253 Validation: 1.802744746208191\n",
      "Epoch  2, CIFAR-10 Batch 1:  Accuracy: Training: 0.25 Validation: 0.3681999444961548 \n",
      "Loss: Training: 2.0594515800476074 Validation: 1.777522325515747\n",
      "Epoch  2, CIFAR-10 Batch 2:  Accuracy: Training: 0.3500000238418579 Validation: 0.38099995255470276 \n",
      "Loss: Training: 1.8943599462509155 Validation: 1.7583191394805908\n",
      "Epoch  2, CIFAR-10 Batch 3:  Accuracy: Training: 0.375 Validation: 0.38839995861053467 \n",
      "Loss: Training: 1.653024435043335 Validation: 1.7347091436386108\n",
      "Epoch  2, CIFAR-10 Batch 4:  Accuracy: Training: 0.375 Validation: 0.39899998903274536 \n",
      "Loss: Training: 1.7600791454315186 Validation: 1.7257035970687866\n",
      "Epoch  2, CIFAR-10 Batch 5:  Accuracy: Training: 0.4000000059604645 Validation: 0.40519997477531433 \n",
      "Loss: Training: 1.7310376167297363 Validation: 1.6943168640136719\n",
      "Epoch  3, CIFAR-10 Batch 1:  Accuracy: Training: 0.32500001788139343 Validation: 0.4115999937057495 \n",
      "Loss: Training: 1.9914216995239258 Validation: 1.6763077974319458\n",
      "Epoch  3, CIFAR-10 Batch 2:  Accuracy: Training: 0.4000000059604645 Validation: 0.4087999761104584 \n",
      "Loss: Training: 1.800487995147705 Validation: 1.6698064804077148\n",
      "Epoch  3, CIFAR-10 Batch 3:  Accuracy: Training: 0.4000000059604645 Validation: 0.4185999631881714 \n",
      "Loss: Training: 1.531327486038208 Validation: 1.6512272357940674\n",
      "Epoch  3, CIFAR-10 Batch 4:  Accuracy: Training: 0.3750000298023224 Validation: 0.42139995098114014 \n",
      "Loss: Training: 1.71078622341156 Validation: 1.6439138650894165\n",
      "Epoch  3, CIFAR-10 Batch 5:  Accuracy: Training: 0.375 Validation: 0.4087999761104584 \n",
      "Loss: Training: 1.674795389175415 Validation: 1.6496151685714722\n",
      "Epoch  4, CIFAR-10 Batch 1:  Accuracy: Training: 0.30000001192092896 Validation: 0.4251999855041504 \n",
      "Loss: Training: 1.9877009391784668 Validation: 1.6367440223693848\n",
      "Epoch  4, CIFAR-10 Batch 2:  Accuracy: Training: 0.40000003576278687 Validation: 0.4253999590873718 \n",
      "Loss: Training: 1.809615969657898 Validation: 1.6248866319656372\n",
      "Epoch  4, CIFAR-10 Batch 3:  Accuracy: Training: 0.42500001192092896 Validation: 0.4243999719619751 \n",
      "Loss: Training: 1.4960441589355469 Validation: 1.6256183385849\n",
      "Epoch  4, CIFAR-10 Batch 4:  Accuracy: Training: 0.45000001788139343 Validation: 0.43139997124671936 \n",
      "Loss: Training: 1.6489746570587158 Validation: 1.603327989578247\n",
      "Epoch  4, CIFAR-10 Batch 5:  Accuracy: Training: 0.30000001192092896 Validation: 0.42159998416900635 \n",
      "Loss: Training: 1.6616554260253906 Validation: 1.6183384656906128\n",
      "Epoch  5, CIFAR-10 Batch 1:  Accuracy: Training: 0.375 Validation: 0.43700000643730164 \n",
      "Loss: Training: 1.8878092765808105 Validation: 1.5924701690673828\n",
      "Epoch  5, CIFAR-10 Batch 2:  Accuracy: Training: 0.375 Validation: 0.44179993867874146 \n",
      "Loss: Training: 1.7576942443847656 Validation: 1.5935769081115723\n",
      "Epoch  5, CIFAR-10 Batch 3:  Accuracy: Training: 0.375 Validation: 0.43439996242523193 \n",
      "Loss: Training: 1.4742984771728516 Validation: 1.6069449186325073\n",
      "Epoch  5, CIFAR-10 Batch 4:  Accuracy: Training: 0.4750000238418579 Validation: 0.437999963760376 \n",
      "Loss: Training: 1.5871832370758057 Validation: 1.5784425735473633\n",
      "Epoch  5, CIFAR-10 Batch 5:  Accuracy: Training: 0.3750000298023224 Validation: 0.44819995760917664 \n",
      "Loss: Training: 1.619599461555481 Validation: 1.568658709526062\n",
      "Epoch  6, CIFAR-10 Batch 1:  Accuracy: Training: 0.4000000059604645 Validation: 0.4265999495983124 \n",
      "Loss: Training: 1.9979064464569092 Validation: 1.6149256229400635\n",
      "Epoch  6, CIFAR-10 Batch 2:  Accuracy: Training: 0.42500001192092896 Validation: 0.4533999562263489 \n",
      "Loss: Training: 1.7545582056045532 Validation: 1.5521131753921509\n",
      "Epoch  6, CIFAR-10 Batch 3:  Accuracy: Training: 0.3999999761581421 Validation: 0.4501999616622925 \n",
      "Loss: Training: 1.40244722366333 Validation: 1.5513195991516113\n",
      "Epoch  6, CIFAR-10 Batch 4:  Accuracy: Training: 0.42500001192092896 Validation: 0.44519999623298645 \n",
      "Loss: Training: 1.5354843139648438 Validation: 1.5514360666275024\n",
      "Epoch  6, CIFAR-10 Batch 5:  Accuracy: Training: 0.40000003576278687 Validation: 0.4501999616622925 \n",
      "Loss: Training: 1.605149507522583 Validation: 1.545853853225708\n",
      "Epoch  7, CIFAR-10 Batch 1:  Accuracy: Training: 0.375 Validation: 0.4453999400138855 \n",
      "Loss: Training: 1.907051682472229 Validation: 1.5401313304901123\n",
      "Epoch  7, CIFAR-10 Batch 2:  Accuracy: Training: 0.40000003576278687 Validation: 0.4593999683856964 \n",
      "Loss: Training: 1.7936604022979736 Validation: 1.5262258052825928\n",
      "Epoch  7, CIFAR-10 Batch 3:  Accuracy: Training: 0.4749999940395355 Validation: 0.4657999575138092 \n",
      "Loss: Training: 1.3778307437896729 Validation: 1.5282411575317383\n",
      "Epoch  7, CIFAR-10 Batch 4:  Accuracy: Training: 0.42500001192092896 Validation: 0.45899996161460876 \n",
      "Loss: Training: 1.5254104137420654 Validation: 1.5325701236724854\n",
      "Epoch  7, CIFAR-10 Batch 5:  Accuracy: Training: 0.3500000238418579 Validation: 0.4415999948978424 \n",
      "Loss: Training: 1.6079132556915283 Validation: 1.5457940101623535\n",
      "Epoch  8, CIFAR-10 Batch 1:  Accuracy: Training: 0.42500001192092896 Validation: 0.46299999952316284 \n",
      "Loss: Training: 1.885491967201233 Validation: 1.5132652521133423\n",
      "Epoch  8, CIFAR-10 Batch 2:  Accuracy: Training: 0.44999998807907104 Validation: 0.4675999879837036 \n",
      "Loss: Training: 1.7163939476013184 Validation: 1.5074653625488281\n",
      "Epoch  8, CIFAR-10 Batch 3:  Accuracy: Training: 0.44999998807907104 Validation: 0.46719998121261597 \n",
      "Loss: Training: 1.362318754196167 Validation: 1.5109094381332397\n",
      "Epoch  8, CIFAR-10 Batch 4:  Accuracy: Training: 0.42500001192092896 Validation: 0.4625999927520752 \n",
      "Loss: Training: 1.5125361680984497 Validation: 1.513919472694397\n",
      "Epoch  8, CIFAR-10 Batch 5:  Accuracy: Training: 0.3750000298023224 Validation: 0.4631999731063843 \n",
      "Loss: Training: 1.573653221130371 Validation: 1.5206913948059082\n",
      "Epoch  9, CIFAR-10 Batch 1:  Accuracy: Training: 0.44999998807907104 Validation: 0.4795999526977539 \n",
      "Loss: Training: 1.8357439041137695 Validation: 1.4879534244537354\n",
      "Epoch  9, CIFAR-10 Batch 2:  Accuracy: Training: 0.40000003576278687 Validation: 0.4649999439716339 \n",
      "Loss: Training: 1.7772109508514404 Validation: 1.5054287910461426\n",
      "Epoch  9, CIFAR-10 Batch 3:  Accuracy: Training: 0.550000011920929 Validation: 0.4771999716758728 \n",
      "Loss: Training: 1.3321740627288818 Validation: 1.4931120872497559\n",
      "Epoch  9, CIFAR-10 Batch 4:  Accuracy: Training: 0.4750000238418579 Validation: 0.4657999873161316 \n",
      "Loss: Training: 1.47352135181427 Validation: 1.5040496587753296\n",
      "Epoch  9, CIFAR-10 Batch 5:  Accuracy: Training: 0.40000003576278687 Validation: 0.4663999676704407 \n",
      "Loss: Training: 1.5401206016540527 Validation: 1.5036355257034302\n",
      "Epoch 10, CIFAR-10 Batch 1:  Accuracy: Training: 0.44999998807907104 Validation: 0.480599969625473 \n",
      "Loss: Training: 1.819230318069458 Validation: 1.4880931377410889\n",
      "Epoch 10, CIFAR-10 Batch 2:  Accuracy: Training: 0.44999998807907104 Validation: 0.4721999764442444 \n",
      "Loss: Training: 1.6738511323928833 Validation: 1.5006006956100464\n",
      "Epoch 10, CIFAR-10 Batch 3:  Accuracy: Training: 0.5250000357627869 Validation: 0.47499996423721313 \n",
      "Loss: Training: 1.3423023223876953 Validation: 1.4898035526275635\n",
      "Epoch 10, CIFAR-10 Batch 4:  Accuracy: Training: 0.45000001788139343 Validation: 0.4649999439716339 \n",
      "Loss: Training: 1.4690155982971191 Validation: 1.5020426511764526\n",
      "Epoch 10, CIFAR-10 Batch 5:  Accuracy: Training: 0.3500000238418579 Validation: 0.4777999520301819 \n",
      "Loss: Training: 1.521869421005249 Validation: 1.4833788871765137\n",
      "Epoch 11, CIFAR-10 Batch 1:  Accuracy: Training: 0.42500001192092896 Validation: 0.47819995880126953 \n",
      "Loss: Training: 1.8277432918548584 Validation: 1.4750038385391235\n",
      "Epoch 11, CIFAR-10 Batch 2:  Accuracy: Training: 0.42500001192092896 Validation: 0.4893999695777893 \n",
      "Loss: Training: 1.681410312652588 Validation: 1.4679114818572998\n",
      "Epoch 11, CIFAR-10 Batch 3:  Accuracy: Training: 0.6000000238418579 Validation: 0.4769999384880066 \n",
      "Loss: Training: 1.2957961559295654 Validation: 1.4704786539077759\n",
      "Epoch 11, CIFAR-10 Batch 4:  Accuracy: Training: 0.42500001192092896 Validation: 0.4857999384403229 \n",
      "Loss: Training: 1.4771772623062134 Validation: 1.4662926197052002\n",
      "Epoch 11, CIFAR-10 Batch 5:  Accuracy: Training: 0.4750000238418579 Validation: 0.4813999533653259 \n",
      "Loss: Training: 1.4838078022003174 Validation: 1.4637598991394043\n",
      "Epoch 12, CIFAR-10 Batch 1:  Accuracy: Training: 0.44999998807907104 Validation: 0.4909999668598175 \n",
      "Loss: Training: 1.773056983947754 Validation: 1.456771731376648\n",
      "Epoch 12, CIFAR-10 Batch 2:  Accuracy: Training: 0.4749999940395355 Validation: 0.49079999327659607 \n",
      "Loss: Training: 1.6165404319763184 Validation: 1.451715111732483\n",
      "Epoch 12, CIFAR-10 Batch 3:  Accuracy: Training: 0.6000000238418579 Validation: 0.4877999722957611 \n",
      "Loss: Training: 1.285914421081543 Validation: 1.454979658126831\n",
      "Epoch 12, CIFAR-10 Batch 4:  Accuracy: Training: 0.45000001788139343 Validation: 0.4745999574661255 \n",
      "Loss: Training: 1.436429738998413 Validation: 1.4648303985595703\n",
      "Epoch 12, CIFAR-10 Batch 5:  Accuracy: Training: 0.4750000238418579 Validation: 0.4793999493122101 \n",
      "Loss: Training: 1.4633407592773438 Validation: 1.4646117687225342\n",
      "Epoch 13, CIFAR-10 Batch 1:  Accuracy: Training: 0.4749999940395355 Validation: 0.491799920797348 \n",
      "Loss: Training: 1.7109748125076294 Validation: 1.4396852254867554\n",
      "Epoch 13, CIFAR-10 Batch 2:  Accuracy: Training: 0.42500001192092896 Validation: 0.48879995942115784 \n",
      "Loss: Training: 1.6321806907653809 Validation: 1.4473322629928589\n",
      "Epoch 13, CIFAR-10 Batch 3:  Accuracy: Training: 0.6000000238418579 Validation: 0.48659995198249817 \n",
      "Loss: Training: 1.299315094947815 Validation: 1.4634039402008057\n",
      "Epoch 13, CIFAR-10 Batch 4:  Accuracy: Training: 0.4750000238418579 Validation: 0.48499995470046997 \n",
      "Loss: Training: 1.4440425634384155 Validation: 1.455474615097046\n",
      "Epoch 13, CIFAR-10 Batch 5:  Accuracy: Training: 0.44999998807907104 Validation: 0.48819994926452637 \n",
      "Loss: Training: 1.4801499843597412 Validation: 1.4457517862319946\n",
      "Epoch 14, CIFAR-10 Batch 1:  Accuracy: Training: 0.42500001192092896 Validation: 0.4907999336719513 \n",
      "Loss: Training: 1.7295230627059937 Validation: 1.4362761974334717\n",
      "Epoch 14, CIFAR-10 Batch 2:  Accuracy: Training: 0.44999998807907104 Validation: 0.4745999574661255 \n",
      "Loss: Training: 1.706499695777893 Validation: 1.4465813636779785\n",
      "Epoch 14, CIFAR-10 Batch 3:  Accuracy: Training: 0.6000000238418579 Validation: 0.49139994382858276 \n",
      "Loss: Training: 1.2650434970855713 Validation: 1.4411256313323975\n",
      "Epoch 14, CIFAR-10 Batch 4:  Accuracy: Training: 0.4750000238418579 Validation: 0.4919999837875366 \n",
      "Loss: Training: 1.402025818824768 Validation: 1.4429174661636353\n",
      "Epoch 14, CIFAR-10 Batch 5:  Accuracy: Training: 0.45000001788139343 Validation: 0.4891999661922455 \n",
      "Loss: Training: 1.4789472818374634 Validation: 1.4453010559082031\n",
      "Epoch 15, CIFAR-10 Batch 1:  Accuracy: Training: 0.5 Validation: 0.48499995470046997 \n",
      "Loss: Training: 1.7038507461547852 Validation: 1.4563937187194824\n",
      "Epoch 15, CIFAR-10 Batch 2:  Accuracy: Training: 0.4750000238418579 Validation: 0.4949999451637268 \n",
      "Loss: Training: 1.5677235126495361 Validation: 1.4373929500579834\n",
      "Epoch 15, CIFAR-10 Batch 3:  Accuracy: Training: 0.550000011920929 Validation: 0.4869999885559082 \n",
      "Loss: Training: 1.2861428260803223 Validation: 1.4363267421722412\n",
      "Epoch 15, CIFAR-10 Batch 4:  Accuracy: Training: 0.4750000238418579 Validation: 0.49399998784065247 \n",
      "Loss: Training: 1.4244296550750732 Validation: 1.4404213428497314\n",
      "Epoch 15, CIFAR-10 Batch 5:  Accuracy: Training: 0.40000003576278687 Validation: 0.48079997301101685 \n",
      "Loss: Training: 1.4717495441436768 Validation: 1.455847978591919\n",
      "Epoch 16, CIFAR-10 Batch 1:  Accuracy: Training: 0.4749999940395355 Validation: 0.5019999742507935 \n",
      "Loss: Training: 1.6874523162841797 Validation: 1.4211596250534058\n",
      "Epoch 16, CIFAR-10 Batch 2:  Accuracy: Training: 0.4749999940395355 Validation: 0.5013999342918396 \n",
      "Loss: Training: 1.539777398109436 Validation: 1.418746829032898\n",
      "Epoch 16, CIFAR-10 Batch 3:  Accuracy: Training: 0.574999988079071 Validation: 0.4909999370574951 \n",
      "Loss: Training: 1.2808822393417358 Validation: 1.435785174369812\n",
      "Epoch 16, CIFAR-10 Batch 4:  Accuracy: Training: 0.4750000238418579 Validation: 0.4991999566555023 \n",
      "Loss: Training: 1.398106336593628 Validation: 1.4161064624786377\n",
      "Epoch 16, CIFAR-10 Batch 5:  Accuracy: Training: 0.42500001192092896 Validation: 0.5003999471664429 \n",
      "Loss: Training: 1.4187953472137451 Validation: 1.4053939580917358\n",
      "Epoch 17, CIFAR-10 Batch 1:  Accuracy: Training: 0.550000011920929 Validation: 0.49779993295669556 \n",
      "Loss: Training: 1.6600704193115234 Validation: 1.4201445579528809\n",
      "Epoch 17, CIFAR-10 Batch 2:  Accuracy: Training: 0.44999998807907104 Validation: 0.5019999742507935 \n",
      "Loss: Training: 1.5019482374191284 Validation: 1.4109282493591309\n",
      "Epoch 17, CIFAR-10 Batch 3:  Accuracy: Training: 0.5750000476837158 Validation: 0.5083999633789062 \n",
      "Loss: Training: 1.2350021600723267 Validation: 1.4077225923538208\n",
      "Epoch 17, CIFAR-10 Batch 4:  Accuracy: Training: 0.4750000238418579 Validation: 0.5047999620437622 \n",
      "Loss: Training: 1.3868610858917236 Validation: 1.4026987552642822\n",
      "Epoch 17, CIFAR-10 Batch 5:  Accuracy: Training: 0.5249999761581421 Validation: 0.5059999227523804 \n",
      "Loss: Training: 1.4223906993865967 Validation: 1.4015806913375854\n",
      "Epoch 18, CIFAR-10 Batch 1:  Accuracy: Training: 0.4749999940395355 Validation: 0.49959996342658997 \n",
      "Loss: Training: 1.6496273279190063 Validation: 1.4091508388519287\n",
      "Epoch 18, CIFAR-10 Batch 2:  Accuracy: Training: 0.40000003576278687 Validation: 0.5097999572753906 \n",
      "Loss: Training: 1.5234973430633545 Validation: 1.4096919298171997\n",
      "Epoch 18, CIFAR-10 Batch 3:  Accuracy: Training: 0.550000011920929 Validation: 0.5021999478340149 \n",
      "Loss: Training: 1.2137418985366821 Validation: 1.3964372873306274\n",
      "Epoch 18, CIFAR-10 Batch 4:  Accuracy: Training: 0.4750000238418579 Validation: 0.5061999559402466 \n",
      "Loss: Training: 1.3775616884231567 Validation: 1.396723985671997\n",
      "Epoch 18, CIFAR-10 Batch 5:  Accuracy: Training: 0.5 Validation: 0.5013999938964844 \n",
      "Loss: Training: 1.4103431701660156 Validation: 1.4025200605392456\n",
      "Epoch 19, CIFAR-10 Batch 1:  Accuracy: Training: 0.4749999940395355 Validation: 0.49879997968673706 \n",
      "Loss: Training: 1.6157358884811401 Validation: 1.4113142490386963\n",
      "Epoch 19, CIFAR-10 Batch 2:  Accuracy: Training: 0.44999998807907104 Validation: 0.5107999444007874 \n",
      "Loss: Training: 1.5076549053192139 Validation: 1.4002751111984253\n",
      "Epoch 19, CIFAR-10 Batch 3:  Accuracy: Training: 0.574999988079071 Validation: 0.5037999153137207 \n",
      "Loss: Training: 1.2187278270721436 Validation: 1.398422122001648\n",
      "Epoch 19, CIFAR-10 Batch 4:  Accuracy: Training: 0.5 Validation: 0.5029999017715454 \n",
      "Loss: Training: 1.3846895694732666 Validation: 1.4124913215637207\n",
      "Epoch 19, CIFAR-10 Batch 5:  Accuracy: Training: 0.5250000357627869 Validation: 0.5023999810218811 \n",
      "Loss: Training: 1.3877079486846924 Validation: 1.4020636081695557\n",
      "Epoch 20, CIFAR-10 Batch 1:  Accuracy: Training: 0.5249999761581421 Validation: 0.5145999193191528 \n",
      "Loss: Training: 1.624911904335022 Validation: 1.389535903930664\n",
      "Epoch 20, CIFAR-10 Batch 2:  Accuracy: Training: 0.42500001192092896 Validation: 0.5057999491691589 \n",
      "Loss: Training: 1.4977128505706787 Validation: 1.4151204824447632\n",
      "Epoch 20, CIFAR-10 Batch 3:  Accuracy: Training: 0.6000000238418579 Validation: 0.511199951171875 \n",
      "Loss: Training: 1.204846739768982 Validation: 1.3967125415802002\n",
      "Epoch 20, CIFAR-10 Batch 4:  Accuracy: Training: 0.5250000357627869 Validation: 0.5069999694824219 \n",
      "Loss: Training: 1.3554961681365967 Validation: 1.397966980934143\n",
      "Epoch 20, CIFAR-10 Batch 5:  Accuracy: Training: 0.5249999761581421 Validation: 0.5049999356269836 \n",
      "Loss: Training: 1.376068115234375 Validation: 1.392217755317688\n",
      "Epoch 21, CIFAR-10 Batch 1:  Accuracy: Training: 0.4749999940395355 Validation: 0.5055999755859375 \n",
      "Loss: Training: 1.6080095767974854 Validation: 1.3923695087432861\n",
      "Epoch 21, CIFAR-10 Batch 2:  Accuracy: Training: 0.45000001788139343 Validation: 0.4999999403953552 \n",
      "Loss: Training: 1.4697511196136475 Validation: 1.396284580230713\n",
      "Epoch 21, CIFAR-10 Batch 3:  Accuracy: Training: 0.6000000238418579 Validation: 0.5013999938964844 \n",
      "Loss: Training: 1.1857376098632812 Validation: 1.412654995918274\n",
      "Epoch 21, CIFAR-10 Batch 4:  Accuracy: Training: 0.4750000238418579 Validation: 0.5051999688148499 \n",
      "Loss: Training: 1.3558807373046875 Validation: 1.3913379907608032\n",
      "Epoch 21, CIFAR-10 Batch 5:  Accuracy: Training: 0.4750000238418579 Validation: 0.5071999430656433 \n",
      "Loss: Training: 1.3848440647125244 Validation: 1.3924636840820312\n",
      "Epoch 22, CIFAR-10 Batch 1:  Accuracy: Training: 0.5249999761581421 Validation: 0.5059999227523804 \n",
      "Loss: Training: 1.5784785747528076 Validation: 1.389944314956665\n",
      "Epoch 22, CIFAR-10 Batch 2:  Accuracy: Training: 0.44999998807907104 Validation: 0.5097999572753906 \n",
      "Loss: Training: 1.4460948705673218 Validation: 1.4099524021148682\n",
      "Epoch 22, CIFAR-10 Batch 3:  Accuracy: Training: 0.6749999523162842 Validation: 0.5165998935699463 \n",
      "Loss: Training: 1.2006170749664307 Validation: 1.3887438774108887\n",
      "Epoch 22, CIFAR-10 Batch 4:  Accuracy: Training: 0.44999998807907104 Validation: 0.5061999559402466 \n",
      "Loss: Training: 1.3970441818237305 Validation: 1.406457543373108\n",
      "Epoch 22, CIFAR-10 Batch 5:  Accuracy: Training: 0.550000011920929 Validation: 0.5133999586105347 \n",
      "Loss: Training: 1.3669623136520386 Validation: 1.3804574012756348\n",
      "Epoch 23, CIFAR-10 Batch 1:  Accuracy: Training: 0.5249999761581421 Validation: 0.5043999552726746 \n",
      "Loss: Training: 1.5694509744644165 Validation: 1.3964780569076538\n",
      "Epoch 23, CIFAR-10 Batch 2:  Accuracy: Training: 0.44999998807907104 Validation: 0.5141999125480652 \n",
      "Loss: Training: 1.4326838254928589 Validation: 1.3798432350158691\n",
      "Epoch 23, CIFAR-10 Batch 3:  Accuracy: Training: 0.625 Validation: 0.511199951171875 \n",
      "Loss: Training: 1.1920050382614136 Validation: 1.3863388299942017\n",
      "Epoch 23, CIFAR-10 Batch 4:  Accuracy: Training: 0.4750000238418579 Validation: 0.5108000040054321 \n",
      "Loss: Training: 1.3502113819122314 Validation: 1.3718342781066895\n",
      "Epoch 23, CIFAR-10 Batch 5:  Accuracy: Training: 0.4750000238418579 Validation: 0.5107999444007874 \n",
      "Loss: Training: 1.3451447486877441 Validation: 1.3729541301727295\n",
      "Epoch 24, CIFAR-10 Batch 1:  Accuracy: Training: 0.4749999940395355 Validation: 0.5103999376296997 \n",
      "Loss: Training: 1.5704345703125 Validation: 1.3778059482574463\n",
      "Epoch 24, CIFAR-10 Batch 2:  Accuracy: Training: 0.44999998807907104 Validation: 0.5141999125480652 \n",
      "Loss: Training: 1.4659614562988281 Validation: 1.384627342224121\n",
      "Epoch 24, CIFAR-10 Batch 3:  Accuracy: Training: 0.6499999761581421 Validation: 0.5119999051094055 \n",
      "Loss: Training: 1.167952537536621 Validation: 1.3755996227264404\n",
      "Epoch 24, CIFAR-10 Batch 4:  Accuracy: Training: 0.42500001192092896 Validation: 0.5057999491691589 \n",
      "Loss: Training: 1.360636591911316 Validation: 1.3953845500946045\n",
      "Epoch 24, CIFAR-10 Batch 5:  Accuracy: Training: 0.4750000238418579 Validation: 0.5117999911308289 \n",
      "Loss: Training: 1.3231451511383057 Validation: 1.375857949256897\n",
      "Epoch 25, CIFAR-10 Batch 1:  Accuracy: Training: 0.5 Validation: 0.5197999477386475 \n",
      "Loss: Training: 1.561987280845642 Validation: 1.3766510486602783\n",
      "Epoch 25, CIFAR-10 Batch 2:  Accuracy: Training: 0.4749999940395355 Validation: 0.50819993019104 \n",
      "Loss: Training: 1.4017776250839233 Validation: 1.3963267803192139\n",
      "Epoch 25, CIFAR-10 Batch 3:  Accuracy: Training: 0.6499999761581421 Validation: 0.4975999593734741 \n",
      "Loss: Training: 1.1469476222991943 Validation: 1.387702465057373\n",
      "Epoch 25, CIFAR-10 Batch 4:  Accuracy: Training: 0.5 Validation: 0.511199951171875 \n",
      "Loss: Training: 1.3338241577148438 Validation: 1.3830500841140747\n",
      "Epoch 25, CIFAR-10 Batch 5:  Accuracy: Training: 0.5 Validation: 0.507599949836731 \n",
      "Loss: Training: 1.3154881000518799 Validation: 1.3665950298309326\n",
      "Epoch 26, CIFAR-10 Batch 1:  Accuracy: Training: 0.5249999761581421 Validation: 0.5095999836921692 \n",
      "Loss: Training: 1.5105977058410645 Validation: 1.3791954517364502\n",
      "Epoch 26, CIFAR-10 Batch 2:  Accuracy: Training: 0.5 Validation: 0.51419997215271 \n",
      "Loss: Training: 1.3655332326889038 Validation: 1.3765672445297241\n",
      "Epoch 26, CIFAR-10 Batch 3:  Accuracy: Training: 0.5750000476837158 Validation: 0.5109999179840088 \n",
      "Loss: Training: 1.172599196434021 Validation: 1.3774199485778809\n",
      "Epoch 26, CIFAR-10 Batch 4:  Accuracy: Training: 0.5250000357627869 Validation: 0.5180000066757202 \n",
      "Loss: Training: 1.3447813987731934 Validation: 1.3672707080841064\n",
      "Epoch 26, CIFAR-10 Batch 5:  Accuracy: Training: 0.4750000238418579 Validation: 0.5183999538421631 \n",
      "Loss: Training: 1.303394079208374 Validation: 1.3578323125839233\n",
      "Epoch 27, CIFAR-10 Batch 1:  Accuracy: Training: 0.42499998211860657 Validation: 0.5067999362945557 \n",
      "Loss: Training: 1.6706862449645996 Validation: 1.4047526121139526\n",
      "Epoch 27, CIFAR-10 Batch 2:  Accuracy: Training: 0.4749999940395355 Validation: 0.5209999680519104 \n",
      "Loss: Training: 1.3659000396728516 Validation: 1.369816780090332\n",
      "Epoch 27, CIFAR-10 Batch 3:  Accuracy: Training: 0.7000000476837158 Validation: 0.5037999153137207 \n",
      "Loss: Training: 1.1310317516326904 Validation: 1.3823171854019165\n",
      "Epoch 27, CIFAR-10 Batch 4:  Accuracy: Training: 0.5750000476837158 Validation: 0.5189999341964722 \n",
      "Loss: Training: 1.360987901687622 Validation: 1.3739190101623535\n",
      "Epoch 27, CIFAR-10 Batch 5:  Accuracy: Training: 0.5249999761581421 Validation: 0.5163999199867249 \n",
      "Loss: Training: 1.3214223384857178 Validation: 1.355783224105835\n",
      "Epoch 28, CIFAR-10 Batch 1:  Accuracy: Training: 0.4749999940395355 Validation: 0.5233999490737915 \n",
      "Loss: Training: 1.5427873134613037 Validation: 1.3498942852020264\n",
      "Epoch 28, CIFAR-10 Batch 2:  Accuracy: Training: 0.550000011920929 Validation: 0.5161999464035034 \n",
      "Loss: Training: 1.337489366531372 Validation: 1.3721344470977783\n",
      "Epoch 28, CIFAR-10 Batch 3:  Accuracy: Training: 0.675000011920929 Validation: 0.5185999870300293 \n",
      "Loss: Training: 1.1699907779693604 Validation: 1.355165958404541\n",
      "Epoch 28, CIFAR-10 Batch 4:  Accuracy: Training: 0.5 Validation: 0.5145999193191528 \n",
      "Loss: Training: 1.3476682901382446 Validation: 1.3565914630889893\n",
      "Epoch 28, CIFAR-10 Batch 5:  Accuracy: Training: 0.5999999642372131 Validation: 0.5235999822616577 \n",
      "Loss: Training: 1.2971500158309937 Validation: 1.3420236110687256\n",
      "Epoch 29, CIFAR-10 Batch 1:  Accuracy: Training: 0.4750000238418579 Validation: 0.5201999545097351 \n",
      "Loss: Training: 1.5517818927764893 Validation: 1.3705923557281494\n",
      "Epoch 29, CIFAR-10 Batch 2:  Accuracy: Training: 0.550000011920929 Validation: 0.5197998881340027 \n",
      "Loss: Training: 1.347963809967041 Validation: 1.3671932220458984\n",
      "Epoch 29, CIFAR-10 Batch 3:  Accuracy: Training: 0.6499999761581421 Validation: 0.5231999754905701 \n",
      "Loss: Training: 1.1532840728759766 Validation: 1.3613970279693604\n",
      "Epoch 29, CIFAR-10 Batch 4:  Accuracy: Training: 0.4750000238418579 Validation: 0.5257999897003174 \n",
      "Loss: Training: 1.327237606048584 Validation: 1.3576618432998657\n",
      "Epoch 29, CIFAR-10 Batch 5:  Accuracy: Training: 0.5250000357627869 Validation: 0.5243999361991882 \n",
      "Loss: Training: 1.3034203052520752 Validation: 1.3546874523162842\n",
      "Epoch 30, CIFAR-10 Batch 1:  Accuracy: Training: 0.5 Validation: 0.520599901676178 \n",
      "Loss: Training: 1.519186019897461 Validation: 1.3473025560379028\n",
      "Epoch 30, CIFAR-10 Batch 2:  Accuracy: Training: 0.5250000357627869 Validation: 0.52239990234375 \n",
      "Loss: Training: 1.3428571224212646 Validation: 1.3522229194641113\n",
      "Epoch 30, CIFAR-10 Batch 3:  Accuracy: Training: 0.6499999761581421 Validation: 0.5225999355316162 \n",
      "Loss: Training: 1.1462324857711792 Validation: 1.3499890565872192\n",
      "Epoch 30, CIFAR-10 Batch 4:  Accuracy: Training: 0.574999988079071 Validation: 0.5221999883651733 \n",
      "Loss: Training: 1.3253426551818848 Validation: 1.356266736984253\n",
      "Epoch 30, CIFAR-10 Batch 5:  Accuracy: Training: 0.5 Validation: 0.517799973487854 \n",
      "Loss: Training: 1.308248519897461 Validation: 1.355785608291626\n",
      "Epoch 31, CIFAR-10 Batch 1:  Accuracy: Training: 0.5 Validation: 0.5185999870300293 \n",
      "Loss: Training: 1.5371644496917725 Validation: 1.367592453956604\n",
      "Epoch 31, CIFAR-10 Batch 2:  Accuracy: Training: 0.550000011920929 Validation: 0.5277999639511108 \n",
      "Loss: Training: 1.351447582244873 Validation: 1.3687176704406738\n",
      "Epoch 31, CIFAR-10 Batch 3:  Accuracy: Training: 0.625 Validation: 0.5225999355316162 \n",
      "Loss: Training: 1.1663627624511719 Validation: 1.3617252111434937\n",
      "Epoch 31, CIFAR-10 Batch 4:  Accuracy: Training: 0.44999998807907104 Validation: 0.5287999510765076 \n",
      "Loss: Training: 1.3273175954818726 Validation: 1.3479740619659424\n",
      "Epoch 31, CIFAR-10 Batch 5:  Accuracy: Training: 0.550000011920929 Validation: 0.5273999571800232 \n",
      "Loss: Training: 1.2645256519317627 Validation: 1.339155673980713\n",
      "Epoch 32, CIFAR-10 Batch 1:  Accuracy: Training: 0.5249999761581421 Validation: 0.5245999693870544 \n",
      "Loss: Training: 1.5467064380645752 Validation: 1.3503848314285278\n",
      "Epoch 32, CIFAR-10 Batch 2:  Accuracy: Training: 0.5250000357627869 Validation: 0.5107998847961426 \n",
      "Loss: Training: 1.3484631776809692 Validation: 1.3861892223358154\n",
      "Epoch 32, CIFAR-10 Batch 3:  Accuracy: Training: 0.625 Validation: 0.5155999660491943 \n",
      "Loss: Training: 1.1762263774871826 Validation: 1.3818044662475586\n",
      "Epoch 32, CIFAR-10 Batch 4:  Accuracy: Training: 0.550000011920929 Validation: 0.5207999348640442 \n",
      "Loss: Training: 1.3098939657211304 Validation: 1.332966685295105\n",
      "Epoch 32, CIFAR-10 Batch 5:  Accuracy: Training: 0.5 Validation: 0.52239990234375 \n",
      "Loss: Training: 1.2646429538726807 Validation: 1.3348687887191772\n",
      "Epoch 33, CIFAR-10 Batch 1:  Accuracy: Training: 0.5 Validation: 0.5281999111175537 \n",
      "Loss: Training: 1.5438318252563477 Validation: 1.3431202173233032\n",
      "Epoch 33, CIFAR-10 Batch 2:  Accuracy: Training: 0.5000000596046448 Validation: 0.5163999795913696 \n",
      "Loss: Training: 1.335415244102478 Validation: 1.3843812942504883\n",
      "Epoch 33, CIFAR-10 Batch 3:  Accuracy: Training: 0.6000000238418579 Validation: 0.5193999409675598 \n",
      "Loss: Training: 1.113175868988037 Validation: 1.3433146476745605\n",
      "Epoch 33, CIFAR-10 Batch 4:  Accuracy: Training: 0.5 Validation: 0.5229999423027039 \n",
      "Loss: Training: 1.304514765739441 Validation: 1.3406963348388672\n",
      "Epoch 33, CIFAR-10 Batch 5:  Accuracy: Training: 0.5249999761581421 Validation: 0.5297999382019043 \n",
      "Loss: Training: 1.2350140810012817 Validation: 1.3359438180923462\n",
      "Epoch 34, CIFAR-10 Batch 1:  Accuracy: Training: 0.5 Validation: 0.533799946308136 \n",
      "Loss: Training: 1.528714656829834 Validation: 1.3423182964324951\n",
      "Epoch 34, CIFAR-10 Batch 2:  Accuracy: Training: 0.5 Validation: 0.5327999591827393 \n",
      "Loss: Training: 1.3091294765472412 Validation: 1.3371330499649048\n",
      "Epoch 34, CIFAR-10 Batch 3:  Accuracy: Training: 0.625 Validation: 0.5245999097824097 \n",
      "Loss: Training: 1.1443034410476685 Validation: 1.344028353691101\n",
      "Epoch 34, CIFAR-10 Batch 4:  Accuracy: Training: 0.550000011920929 Validation: 0.5233999490737915 \n",
      "Loss: Training: 1.3309690952301025 Validation: 1.340064287185669\n",
      "Epoch 34, CIFAR-10 Batch 5:  Accuracy: Training: 0.5 Validation: 0.5273998975753784 \n",
      "Loss: Training: 1.2768309116363525 Validation: 1.343217134475708\n",
      "Epoch 35, CIFAR-10 Batch 1:  Accuracy: Training: 0.5249999761581421 Validation: 0.5215999484062195 \n",
      "Loss: Training: 1.493833065032959 Validation: 1.3579941987991333\n",
      "Epoch 35, CIFAR-10 Batch 2:  Accuracy: Training: 0.5250000357627869 Validation: 0.5281999707221985 \n",
      "Loss: Training: 1.3233858346939087 Validation: 1.3308565616607666\n",
      "Epoch 35, CIFAR-10 Batch 3:  Accuracy: Training: 0.6499999761581421 Validation: 0.5227999091148376 \n",
      "Loss: Training: 1.1035830974578857 Validation: 1.3296730518341064\n",
      "Epoch 35, CIFAR-10 Batch 4:  Accuracy: Training: 0.5249999761581421 Validation: 0.5237999558448792 \n",
      "Loss: Training: 1.311779499053955 Validation: 1.3326263427734375\n",
      "Epoch 35, CIFAR-10 Batch 5:  Accuracy: Training: 0.5 Validation: 0.5221999287605286 \n",
      "Loss: Training: 1.242537021636963 Validation: 1.3280580043792725\n",
      "Epoch 36, CIFAR-10 Batch 1:  Accuracy: Training: 0.5249999761581421 Validation: 0.5179999470710754 \n",
      "Loss: Training: 1.49695885181427 Validation: 1.3483455181121826\n",
      "Epoch 36, CIFAR-10 Batch 2:  Accuracy: Training: 0.4749999940395355 Validation: 0.5241999626159668 \n",
      "Loss: Training: 1.330173134803772 Validation: 1.3494577407836914\n",
      "Epoch 36, CIFAR-10 Batch 3:  Accuracy: Training: 0.675000011920929 Validation: 0.5273999571800232 \n",
      "Loss: Training: 1.1369332075119019 Validation: 1.3337640762329102\n",
      "Epoch 36, CIFAR-10 Batch 4:  Accuracy: Training: 0.5250000357627869 Validation: 0.5219999551773071 \n",
      "Loss: Training: 1.3669989109039307 Validation: 1.3508191108703613\n",
      "Epoch 36, CIFAR-10 Batch 5:  Accuracy: Training: 0.5249999761581421 Validation: 0.5259999632835388 \n",
      "Loss: Training: 1.2550885677337646 Validation: 1.326380729675293\n",
      "Epoch 37, CIFAR-10 Batch 1:  Accuracy: Training: 0.5249999761581421 Validation: 0.5225999355316162 \n",
      "Loss: Training: 1.462180733680725 Validation: 1.352912425994873\n",
      "Epoch 37, CIFAR-10 Batch 2:  Accuracy: Training: 0.5 Validation: 0.520599901676178 \n",
      "Loss: Training: 1.3236929178237915 Validation: 1.3576316833496094\n",
      "Epoch 37, CIFAR-10 Batch 3:  Accuracy: Training: 0.6499999761581421 Validation: 0.5263999700546265 \n",
      "Loss: Training: 1.1259022951126099 Validation: 1.3385777473449707\n",
      "Epoch 37, CIFAR-10 Batch 4:  Accuracy: Training: 0.5 Validation: 0.5333999991416931 \n",
      "Loss: Training: 1.2985121011734009 Validation: 1.3328211307525635\n",
      "Epoch 37, CIFAR-10 Batch 5:  Accuracy: Training: 0.550000011920929 Validation: 0.5231999754905701 \n",
      "Loss: Training: 1.2298955917358398 Validation: 1.323479413986206\n",
      "Epoch 38, CIFAR-10 Batch 1:  Accuracy: Training: 0.5249999761581421 Validation: 0.5193999409675598 \n",
      "Loss: Training: 1.4678874015808105 Validation: 1.3418951034545898\n",
      "Epoch 38, CIFAR-10 Batch 2:  Accuracy: Training: 0.5250000357627869 Validation: 0.5261999368667603 \n",
      "Loss: Training: 1.2998261451721191 Validation: 1.3618189096450806\n",
      "Epoch 38, CIFAR-10 Batch 3:  Accuracy: Training: 0.625 Validation: 0.5267999768257141 \n",
      "Loss: Training: 1.127178430557251 Validation: 1.3420069217681885\n",
      "Epoch 38, CIFAR-10 Batch 4:  Accuracy: Training: 0.550000011920929 Validation: 0.5365999341011047 \n",
      "Loss: Training: 1.3049724102020264 Validation: 1.3142757415771484\n",
      "Epoch 38, CIFAR-10 Batch 5:  Accuracy: Training: 0.550000011920929 Validation: 0.5327998995780945 \n",
      "Loss: Training: 1.241060733795166 Validation: 1.3114683628082275\n",
      "Epoch 39, CIFAR-10 Batch 1:  Accuracy: Training: 0.5 Validation: 0.5183999538421631 \n",
      "Loss: Training: 1.4568052291870117 Validation: 1.3405364751815796\n",
      "Epoch 39, CIFAR-10 Batch 2:  Accuracy: Training: 0.4999999701976776 Validation: 0.5317999720573425 \n",
      "Loss: Training: 1.2697362899780273 Validation: 1.3315606117248535\n",
      "Epoch 39, CIFAR-10 Batch 3:  Accuracy: Training: 0.675000011920929 Validation: 0.5275999307632446 \n",
      "Loss: Training: 1.126625657081604 Validation: 1.3642979860305786\n",
      "Epoch 39, CIFAR-10 Batch 4:  Accuracy: Training: 0.5750000476837158 Validation: 0.5311999320983887 \n",
      "Loss: Training: 1.3002121448516846 Validation: 1.3240975141525269\n",
      "Epoch 39, CIFAR-10 Batch 5:  Accuracy: Training: 0.550000011920929 Validation: 0.531999945640564 \n",
      "Loss: Training: 1.2434587478637695 Validation: 1.3156940937042236\n",
      "Epoch 40, CIFAR-10 Batch 1:  Accuracy: Training: 0.5 Validation: 0.5207999348640442 \n",
      "Loss: Training: 1.4565176963806152 Validation: 1.3275632858276367\n",
      "Epoch 40, CIFAR-10 Batch 2:  Accuracy: Training: 0.4749999940395355 Validation: 0.5295999646186829 \n",
      "Loss: Training: 1.2811278104782104 Validation: 1.33380126953125\n",
      "Epoch 40, CIFAR-10 Batch 3:  Accuracy: Training: 0.6499999761581421 Validation: 0.5311999320983887 \n",
      "Loss: Training: 1.1119017601013184 Validation: 1.3197999000549316\n",
      "Epoch 40, CIFAR-10 Batch 4:  Accuracy: Training: 0.5 Validation: 0.5299999117851257 \n",
      "Loss: Training: 1.3060182332992554 Validation: 1.3244049549102783\n",
      "Epoch 40, CIFAR-10 Batch 5:  Accuracy: Training: 0.550000011920929 Validation: 0.5347999334335327 \n",
      "Loss: Training: 1.2560484409332275 Validation: 1.316004991531372\n",
      "Epoch 41, CIFAR-10 Batch 1:  Accuracy: Training: 0.4750000238418579 Validation: 0.528999924659729 \n",
      "Loss: Training: 1.413419246673584 Validation: 1.3151023387908936\n",
      "Epoch 41, CIFAR-10 Batch 2:  Accuracy: Training: 0.5250000357627869 Validation: 0.5333999395370483 \n",
      "Loss: Training: 1.2925583124160767 Validation: 1.3379871845245361\n",
      "Epoch 41, CIFAR-10 Batch 3:  Accuracy: Training: 0.675000011920929 Validation: 0.5313999652862549 \n",
      "Loss: Training: 1.0838168859481812 Validation: 1.315938949584961\n",
      "Epoch 41, CIFAR-10 Batch 4:  Accuracy: Training: 0.550000011920929 Validation: 0.533799946308136 \n",
      "Loss: Training: 1.3024876117706299 Validation: 1.3249760866165161\n",
      "Epoch 41, CIFAR-10 Batch 5:  Accuracy: Training: 0.5 Validation: 0.5263999104499817 \n",
      "Loss: Training: 1.2453134059906006 Validation: 1.323452115058899\n",
      "Epoch 42, CIFAR-10 Batch 1:  Accuracy: Training: 0.5249999761581421 Validation: 0.5281999707221985 \n",
      "Loss: Training: 1.4372830390930176 Validation: 1.3186004161834717\n",
      "Epoch 42, CIFAR-10 Batch 2:  Accuracy: Training: 0.550000011920929 Validation: 0.5287998914718628 \n",
      "Loss: Training: 1.2718138694763184 Validation: 1.3363267183303833\n",
      "Epoch 42, CIFAR-10 Batch 3:  Accuracy: Training: 0.6499999761581421 Validation: 0.5227999687194824 \n",
      "Loss: Training: 1.1230350732803345 Validation: 1.3438717126846313\n",
      "Epoch 42, CIFAR-10 Batch 4:  Accuracy: Training: 0.4749999940395355 Validation: 0.5349999666213989 \n",
      "Loss: Training: 1.3176243305206299 Validation: 1.319201946258545\n",
      "Epoch 42, CIFAR-10 Batch 5:  Accuracy: Training: 0.5249999761581421 Validation: 0.525399923324585 \n",
      "Loss: Training: 1.2694194316864014 Validation: 1.3287948369979858\n",
      "Epoch 43, CIFAR-10 Batch 1:  Accuracy: Training: 0.4750000238418579 Validation: 0.5291999578475952 \n",
      "Loss: Training: 1.4181475639343262 Validation: 1.3232755661010742\n",
      "Epoch 43, CIFAR-10 Batch 2:  Accuracy: Training: 0.5249999761581421 Validation: 0.5229999423027039 \n",
      "Loss: Training: 1.277862548828125 Validation: 1.3500125408172607\n",
      "Epoch 43, CIFAR-10 Batch 3:  Accuracy: Training: 0.675000011920929 Validation: 0.5321999192237854 \n",
      "Loss: Training: 1.1243739128112793 Validation: 1.3248282670974731\n",
      "Epoch 43, CIFAR-10 Batch 4:  Accuracy: Training: 0.574999988079071 Validation: 0.5293999314308167 \n",
      "Loss: Training: 1.3063949346542358 Validation: 1.3165949583053589\n",
      "Epoch 43, CIFAR-10 Batch 5:  Accuracy: Training: 0.5 Validation: 0.5287999510765076 \n",
      "Loss: Training: 1.2759888172149658 Validation: 1.3280847072601318\n",
      "Epoch 44, CIFAR-10 Batch 1:  Accuracy: Training: 0.550000011920929 Validation: 0.531999945640564 \n",
      "Loss: Training: 1.4630763530731201 Validation: 1.335266351699829\n",
      "Epoch 44, CIFAR-10 Batch 2:  Accuracy: Training: 0.5750000476837158 Validation: 0.5333999395370483 \n",
      "Loss: Training: 1.2649885416030884 Validation: 1.3359767198562622\n",
      "Epoch 44, CIFAR-10 Batch 3:  Accuracy: Training: 0.6499999761581421 Validation: 0.5321999788284302 \n",
      "Loss: Training: 1.0998690128326416 Validation: 1.315476417541504\n",
      "Epoch 44, CIFAR-10 Batch 4:  Accuracy: Training: 0.5 Validation: 0.5347999334335327 \n",
      "Loss: Training: 1.3153376579284668 Validation: 1.3145110607147217\n",
      "Epoch 44, CIFAR-10 Batch 5:  Accuracy: Training: 0.550000011920929 Validation: 0.5283999443054199 \n",
      "Loss: Training: 1.2359230518341064 Validation: 1.3227484226226807\n",
      "Epoch 45, CIFAR-10 Batch 1:  Accuracy: Training: 0.5 Validation: 0.5361999273300171 \n",
      "Loss: Training: 1.4109357595443726 Validation: 1.3116832971572876\n",
      "Epoch 45, CIFAR-10 Batch 2:  Accuracy: Training: 0.550000011920929 Validation: 0.5397999286651611 \n",
      "Loss: Training: 1.2670226097106934 Validation: 1.330693244934082\n",
      "Epoch 45, CIFAR-10 Batch 3:  Accuracy: Training: 0.675000011920929 Validation: 0.53739994764328 \n",
      "Loss: Training: 1.0929254293441772 Validation: 1.3045392036437988\n",
      "Epoch 45, CIFAR-10 Batch 4:  Accuracy: Training: 0.5 Validation: 0.532599925994873 \n",
      "Loss: Training: 1.3101191520690918 Validation: 1.3370023965835571\n",
      "Epoch 45, CIFAR-10 Batch 5:  Accuracy: Training: 0.550000011920929 Validation: 0.5359998941421509 \n",
      "Loss: Training: 1.232935905456543 Validation: 1.3205013275146484\n",
      "Epoch 46, CIFAR-10 Batch 1:  Accuracy: Training: 0.5 Validation: 0.5357999205589294 \n",
      "Loss: Training: 1.403255581855774 Validation: 1.3121834993362427\n",
      "Epoch 46, CIFAR-10 Batch 2:  Accuracy: Training: 0.574999988079071 Validation: 0.5361999869346619 \n",
      "Loss: Training: 1.2663981914520264 Validation: 1.331321358680725\n",
      "Epoch 46, CIFAR-10 Batch 3:  Accuracy: Training: 0.7000000476837158 Validation: 0.5379999279975891 \n",
      "Loss: Training: 1.0749191045761108 Validation: 1.3149980306625366\n",
      "Epoch 46, CIFAR-10 Batch 4:  Accuracy: Training: 0.5249999761581421 Validation: 0.5351999402046204 \n",
      "Loss: Training: 1.2890825271606445 Validation: 1.324438214302063\n",
      "Epoch 46, CIFAR-10 Batch 5:  Accuracy: Training: 0.550000011920929 Validation: 0.5307999849319458 \n",
      "Loss: Training: 1.2422289848327637 Validation: 1.3178845643997192\n",
      "Epoch 47, CIFAR-10 Batch 1:  Accuracy: Training: 0.5249999761581421 Validation: 0.5381999611854553 \n",
      "Loss: Training: 1.427717685699463 Validation: 1.3289868831634521\n",
      "Epoch 47, CIFAR-10 Batch 2:  Accuracy: Training: 0.5 Validation: 0.5313999056816101 \n",
      "Loss: Training: 1.2662882804870605 Validation: 1.3424038887023926\n",
      "Epoch 47, CIFAR-10 Batch 3:  Accuracy: Training: 0.675000011920929 Validation: 0.5369999408721924 \n",
      "Loss: Training: 1.1247667074203491 Validation: 1.3090839385986328\n",
      "Epoch 47, CIFAR-10 Batch 4:  Accuracy: Training: 0.5750000476837158 Validation: 0.5387998819351196 \n",
      "Loss: Training: 1.3090007305145264 Validation: 1.3112971782684326\n",
      "Epoch 47, CIFAR-10 Batch 5:  Accuracy: Training: 0.550000011920929 Validation: 0.5353999137878418 \n",
      "Loss: Training: 1.2256503105163574 Validation: 1.3048441410064697\n",
      "Epoch 48, CIFAR-10 Batch 1:  Accuracy: Training: 0.5249999761581421 Validation: 0.5349999666213989 \n",
      "Loss: Training: 1.417209506034851 Validation: 1.3210411071777344\n",
      "Epoch 48, CIFAR-10 Batch 2:  Accuracy: Training: 0.550000011920929 Validation: 0.5323999524116516 \n",
      "Loss: Training: 1.277803659439087 Validation: 1.3409409523010254\n",
      "Epoch 48, CIFAR-10 Batch 3:  Accuracy: Training: 0.6499999761581421 Validation: 0.5385999083518982 \n",
      "Loss: Training: 1.1092922687530518 Validation: 1.3046245574951172\n",
      "Epoch 48, CIFAR-10 Batch 4:  Accuracy: Training: 0.5250000357627869 Validation: 0.5395999550819397 \n",
      "Loss: Training: 1.246856689453125 Validation: 1.3155796527862549\n",
      "Epoch 48, CIFAR-10 Batch 5:  Accuracy: Training: 0.5 Validation: 0.5383999347686768 \n",
      "Loss: Training: 1.2311372756958008 Validation: 1.3043166399002075\n",
      "Epoch 49, CIFAR-10 Batch 1:  Accuracy: Training: 0.5 Validation: 0.5393999814987183 \n",
      "Loss: Training: 1.3954006433486938 Validation: 1.2999768257141113\n",
      "Epoch 49, CIFAR-10 Batch 2:  Accuracy: Training: 0.574999988079071 Validation: 0.5299999713897705 \n",
      "Loss: Training: 1.253522515296936 Validation: 1.3340845108032227\n",
      "Epoch 49, CIFAR-10 Batch 3:  Accuracy: Training: 0.6499999761581421 Validation: 0.5375999212265015 \n",
      "Loss: Training: 1.0999666452407837 Validation: 1.2986807823181152\n",
      "Epoch 49, CIFAR-10 Batch 4:  Accuracy: Training: 0.5 Validation: 0.535599946975708 \n",
      "Loss: Training: 1.2848014831542969 Validation: 1.3213058710098267\n",
      "Epoch 49, CIFAR-10 Batch 5:  Accuracy: Training: 0.5250000357627869 Validation: 0.5343999862670898 \n",
      "Loss: Training: 1.2074235677719116 Validation: 1.308712124824524\n",
      "Epoch 50, CIFAR-10 Batch 1:  Accuracy: Training: 0.4750000238418579 Validation: 0.5369999408721924 \n",
      "Loss: Training: 1.3872348070144653 Validation: 1.314889907836914\n",
      "Epoch 50, CIFAR-10 Batch 2:  Accuracy: Training: 0.5249999761581421 Validation: 0.5309999585151672 \n",
      "Loss: Training: 1.2707363367080688 Validation: 1.343326449394226\n",
      "Epoch 50, CIFAR-10 Batch 3:  Accuracy: Training: 0.6499999761581421 Validation: 0.5333999395370483 \n",
      "Loss: Training: 1.1166929006576538 Validation: 1.3169207572937012\n",
      "Epoch 50, CIFAR-10 Batch 4:  Accuracy: Training: 0.6000000238418579 Validation: 0.5395999550819397 \n",
      "Loss: Training: 1.2405319213867188 Validation: 1.3021918535232544\n",
      "Epoch 50, CIFAR-10 Batch 5:  Accuracy: Training: 0.5 Validation: 0.5323998928070068 \n",
      "Loss: Training: 1.2067773342132568 Validation: 1.3026314973831177\n",
      "Epoch 51, CIFAR-10 Batch 1:  Accuracy: Training: 0.5 Validation: 0.5427999496459961 \n",
      "Loss: Training: 1.4246268272399902 Validation: 1.3102226257324219\n",
      "Epoch 51, CIFAR-10 Batch 2:  Accuracy: Training: 0.550000011920929 Validation: 0.5381999015808105 \n",
      "Loss: Training: 1.2679884433746338 Validation: 1.3133834600448608\n",
      "Epoch 51, CIFAR-10 Batch 3:  Accuracy: Training: 0.6499999761581421 Validation: 0.5343999266624451 \n",
      "Loss: Training: 1.1057590246200562 Validation: 1.3090060949325562\n",
      "Epoch 51, CIFAR-10 Batch 4:  Accuracy: Training: 0.550000011920929 Validation: 0.539199948310852 \n",
      "Loss: Training: 1.2877132892608643 Validation: 1.3131952285766602\n",
      "Epoch 51, CIFAR-10 Batch 5:  Accuracy: Training: 0.574999988079071 Validation: 0.5363999009132385 \n",
      "Loss: Training: 1.2286971807479858 Validation: 1.301031231880188\n",
      "Epoch 52, CIFAR-10 Batch 1:  Accuracy: Training: 0.550000011920929 Validation: 0.53659987449646 \n",
      "Loss: Training: 1.37967050075531 Validation: 1.3026750087738037\n",
      "Epoch 52, CIFAR-10 Batch 2:  Accuracy: Training: 0.550000011920929 Validation: 0.5315999388694763 \n",
      "Loss: Training: 1.2405940294265747 Validation: 1.3148255348205566\n",
      "Epoch 52, CIFAR-10 Batch 3:  Accuracy: Training: 0.625 Validation: 0.528999924659729 \n",
      "Loss: Training: 1.1212280988693237 Validation: 1.3181697130203247\n",
      "Epoch 52, CIFAR-10 Batch 4:  Accuracy: Training: 0.5750000476837158 Validation: 0.540399968624115 \n",
      "Loss: Training: 1.2639905214309692 Validation: 1.3090565204620361\n",
      "Epoch 52, CIFAR-10 Batch 5:  Accuracy: Training: 0.5750000476837158 Validation: 0.5309998989105225 \n",
      "Loss: Training: 1.2185771465301514 Validation: 1.3154587745666504\n",
      "Epoch 53, CIFAR-10 Batch 1:  Accuracy: Training: 0.4750000238418579 Validation: 0.5425999164581299 \n",
      "Loss: Training: 1.3860509395599365 Validation: 1.3061213493347168\n",
      "Epoch 53, CIFAR-10 Batch 2:  Accuracy: Training: 0.550000011920929 Validation: 0.5341999530792236 \n",
      "Loss: Training: 1.2467573881149292 Validation: 1.3189622163772583\n",
      "Epoch 53, CIFAR-10 Batch 3:  Accuracy: Training: 0.6499999761581421 Validation: 0.5413999557495117 \n",
      "Loss: Training: 1.0815088748931885 Validation: 1.295957088470459\n",
      "Epoch 53, CIFAR-10 Batch 4:  Accuracy: Training: 0.5250000357627869 Validation: 0.5473998785018921 \n",
      "Loss: Training: 1.2548673152923584 Validation: 1.301873803138733\n",
      "Epoch 53, CIFAR-10 Batch 5:  Accuracy: Training: 0.5499999523162842 Validation: 0.5389999151229858 \n",
      "Loss: Training: 1.1966462135314941 Validation: 1.296311855316162\n",
      "Epoch 54, CIFAR-10 Batch 1:  Accuracy: Training: 0.5750000476837158 Validation: 0.5265999436378479 \n",
      "Loss: Training: 1.3912394046783447 Validation: 1.344657301902771\n",
      "Epoch 54, CIFAR-10 Batch 2:  Accuracy: Training: 0.6000000238418579 Validation: 0.5311999320983887 \n",
      "Loss: Training: 1.2629780769348145 Validation: 1.3378255367279053\n",
      "Epoch 54, CIFAR-10 Batch 3:  Accuracy: Training: 0.6499999761581421 Validation: 0.5419999361038208 \n",
      "Loss: Training: 1.083315134048462 Validation: 1.2952568531036377\n",
      "Epoch 54, CIFAR-10 Batch 4:  Accuracy: Training: 0.550000011920929 Validation: 0.5417999029159546 \n",
      "Loss: Training: 1.2202465534210205 Validation: 1.2986948490142822\n",
      "Epoch 54, CIFAR-10 Batch 5:  Accuracy: Training: 0.574999988079071 Validation: 0.5377998948097229 \n",
      "Loss: Training: 1.221755027770996 Validation: 1.2954494953155518\n",
      "Epoch 55, CIFAR-10 Batch 1:  Accuracy: Training: 0.4750000238418579 Validation: 0.5397999286651611 \n",
      "Loss: Training: 1.3848516941070557 Validation: 1.3059418201446533\n",
      "Epoch 55, CIFAR-10 Batch 2:  Accuracy: Training: 0.6000000238418579 Validation: 0.5369999408721924 \n",
      "Loss: Training: 1.2696781158447266 Validation: 1.3209865093231201\n",
      "Epoch 55, CIFAR-10 Batch 3:  Accuracy: Training: 0.6499999761581421 Validation: 0.5265999436378479 \n",
      "Loss: Training: 1.1291512250900269 Validation: 1.3542476892471313\n",
      "Epoch 55, CIFAR-10 Batch 4:  Accuracy: Training: 0.550000011920929 Validation: 0.5465999245643616 \n",
      "Loss: Training: 1.2563972473144531 Validation: 1.2969141006469727\n",
      "Epoch 55, CIFAR-10 Batch 5:  Accuracy: Training: 0.574999988079071 Validation: 0.5349999070167542 \n",
      "Loss: Training: 1.2135069370269775 Validation: 1.3079873323440552\n",
      "Epoch 56, CIFAR-10 Batch 1:  Accuracy: Training: 0.5 Validation: 0.5303999185562134 \n",
      "Loss: Training: 1.3874192237854004 Validation: 1.3192495107650757\n",
      "Epoch 56, CIFAR-10 Batch 2:  Accuracy: Training: 0.5750000476837158 Validation: 0.5471999645233154 \n",
      "Loss: Training: 1.2559353113174438 Validation: 1.306547999382019\n",
      "Epoch 56, CIFAR-10 Batch 3:  Accuracy: Training: 0.6000000238418579 Validation: 0.5391998887062073 \n",
      "Loss: Training: 1.0894687175750732 Validation: 1.287030816078186\n",
      "Epoch 56, CIFAR-10 Batch 4:  Accuracy: Training: 0.5250000357627869 Validation: 0.5443999171257019 \n",
      "Loss: Training: 1.2570621967315674 Validation: 1.2882063388824463\n",
      "Epoch 56, CIFAR-10 Batch 5:  Accuracy: Training: 0.550000011920929 Validation: 0.5343999266624451 \n",
      "Loss: Training: 1.2415392398834229 Validation: 1.3030403852462769\n",
      "Epoch 57, CIFAR-10 Batch 1:  Accuracy: Training: 0.550000011920929 Validation: 0.5375999212265015 \n",
      "Loss: Training: 1.3706862926483154 Validation: 1.316648244857788\n",
      "Epoch 57, CIFAR-10 Batch 2:  Accuracy: Training: 0.5750000476837158 Validation: 0.5431999564170837 \n",
      "Loss: Training: 1.25548255443573 Validation: 1.3069193363189697\n",
      "Epoch 57, CIFAR-10 Batch 3:  Accuracy: Training: 0.625 Validation: 0.5407999157905579 \n",
      "Loss: Training: 1.115572214126587 Validation: 1.3006078004837036\n",
      "Epoch 57, CIFAR-10 Batch 4:  Accuracy: Training: 0.6000000238418579 Validation: 0.5463999509811401 \n",
      "Loss: Training: 1.2590268850326538 Validation: 1.290536642074585\n",
      "Epoch 57, CIFAR-10 Batch 5:  Accuracy: Training: 0.550000011920929 Validation: 0.5393999218940735 \n",
      "Loss: Training: 1.1862884759902954 Validation: 1.289027452468872\n",
      "Epoch 58, CIFAR-10 Batch 1:  Accuracy: Training: 0.4749999940395355 Validation: 0.5527999401092529 \n",
      "Loss: Training: 1.3806991577148438 Validation: 1.2897316217422485\n",
      "Epoch 58, CIFAR-10 Batch 2:  Accuracy: Training: 0.550000011920929 Validation: 0.5429999232292175 \n",
      "Loss: Training: 1.225894570350647 Validation: 1.2905683517456055\n",
      "Epoch 58, CIFAR-10 Batch 3:  Accuracy: Training: 0.625 Validation: 0.5417999029159546 \n",
      "Loss: Training: 1.0947009325027466 Validation: 1.2968860864639282\n",
      "Epoch 58, CIFAR-10 Batch 4:  Accuracy: Training: 0.5249999761581421 Validation: 0.5441999435424805 \n",
      "Loss: Training: 1.306330680847168 Validation: 1.2983101606369019\n",
      "Epoch 58, CIFAR-10 Batch 5:  Accuracy: Training: 0.6000000238418579 Validation: 0.5415999293327332 \n",
      "Loss: Training: 1.2087762355804443 Validation: 1.3002417087554932\n",
      "Epoch 59, CIFAR-10 Batch 1:  Accuracy: Training: 0.5249999761581421 Validation: 0.5447999238967896 \n",
      "Loss: Training: 1.3658866882324219 Validation: 1.293511986732483\n",
      "Epoch 59, CIFAR-10 Batch 2:  Accuracy: Training: 0.550000011920929 Validation: 0.5343999266624451 \n",
      "Loss: Training: 1.2353174686431885 Validation: 1.3230092525482178\n",
      "Epoch 59, CIFAR-10 Batch 3:  Accuracy: Training: 0.625 Validation: 0.539199948310852 \n",
      "Loss: Training: 1.084154486656189 Validation: 1.3020961284637451\n",
      "Epoch 59, CIFAR-10 Batch 4:  Accuracy: Training: 0.5750000476837158 Validation: 0.5531999468803406 \n",
      "Loss: Training: 1.2556763887405396 Validation: 1.2916841506958008\n",
      "Epoch 59, CIFAR-10 Batch 5:  Accuracy: Training: 0.5750000476837158 Validation: 0.5411999225616455 \n",
      "Loss: Training: 1.2056872844696045 Validation: 1.2884193658828735\n",
      "Epoch 60, CIFAR-10 Batch 1:  Accuracy: Training: 0.5 Validation: 0.5411999821662903 \n",
      "Loss: Training: 1.3523640632629395 Validation: 1.3108739852905273\n",
      "Epoch 60, CIFAR-10 Batch 2:  Accuracy: Training: 0.625 Validation: 0.5383999347686768 \n",
      "Loss: Training: 1.2432684898376465 Validation: 1.310321569442749\n",
      "Epoch 60, CIFAR-10 Batch 3:  Accuracy: Training: 0.625 Validation: 0.5449999570846558 \n",
      "Loss: Training: 1.0745933055877686 Validation: 1.2849384546279907\n",
      "Epoch 60, CIFAR-10 Batch 4:  Accuracy: Training: 0.550000011920929 Validation: 0.5363999605178833 \n",
      "Loss: Training: 1.255204439163208 Validation: 1.3015271425247192\n",
      "Epoch 60, CIFAR-10 Batch 5:  Accuracy: Training: 0.6000000238418579 Validation: 0.5365999341011047 \n",
      "Loss: Training: 1.1928093433380127 Validation: 1.2879290580749512\n",
      "Epoch 61, CIFAR-10 Batch 1:  Accuracy: Training: 0.5249999761581421 Validation: 0.5481999516487122 \n",
      "Loss: Training: 1.350513219833374 Validation: 1.304932951927185\n",
      "Epoch 61, CIFAR-10 Batch 2:  Accuracy: Training: 0.5750000476837158 Validation: 0.5423999428749084 \n",
      "Loss: Training: 1.2195054292678833 Validation: 1.3082835674285889\n",
      "Epoch 61, CIFAR-10 Batch 3:  Accuracy: Training: 0.625 Validation: 0.5349999666213989 \n",
      "Loss: Training: 1.0829639434814453 Validation: 1.3145577907562256\n",
      "Epoch 61, CIFAR-10 Batch 4:  Accuracy: Training: 0.550000011920929 Validation: 0.5387999415397644 \n",
      "Loss: Training: 1.268301010131836 Validation: 1.3080353736877441\n",
      "Epoch 61, CIFAR-10 Batch 5:  Accuracy: Training: 0.6000000238418579 Validation: 0.5343999266624451 \n",
      "Loss: Training: 1.2011747360229492 Validation: 1.2931815385818481\n",
      "Epoch 62, CIFAR-10 Batch 1:  Accuracy: Training: 0.5249999761581421 Validation: 0.5439999103546143 \n",
      "Loss: Training: 1.3393522500991821 Validation: 1.2945783138275146\n",
      "Epoch 62, CIFAR-10 Batch 2:  Accuracy: Training: 0.625 Validation: 0.5431999564170837 \n",
      "Loss: Training: 1.2203569412231445 Validation: 1.3093879222869873\n",
      "Epoch 62, CIFAR-10 Batch 3:  Accuracy: Training: 0.625 Validation: 0.5421999096870422 \n",
      "Loss: Training: 1.0856482982635498 Validation: 1.3021482229232788\n",
      "Epoch 62, CIFAR-10 Batch 4:  Accuracy: Training: 0.5750000476837158 Validation: 0.5445999503135681 \n",
      "Loss: Training: 1.2006502151489258 Validation: 1.2844146490097046\n",
      "Epoch 62, CIFAR-10 Batch 5:  Accuracy: Training: 0.5750000476837158 Validation: 0.5401999354362488 \n",
      "Loss: Training: 1.205072283744812 Validation: 1.2938499450683594\n",
      "Epoch 63, CIFAR-10 Batch 1:  Accuracy: Training: 0.5 Validation: 0.5359999537467957 \n",
      "Loss: Training: 1.3939130306243896 Validation: 1.3063709735870361\n",
      "Epoch 63, CIFAR-10 Batch 2:  Accuracy: Training: 0.5250000357627869 Validation: 0.5409999489784241 \n",
      "Loss: Training: 1.2401103973388672 Validation: 1.3173508644104004\n",
      "Epoch 63, CIFAR-10 Batch 3:  Accuracy: Training: 0.7000000476837158 Validation: 0.5429999232292175 \n",
      "Loss: Training: 1.073315978050232 Validation: 1.2854844331741333\n",
      "Epoch 63, CIFAR-10 Batch 4:  Accuracy: Training: 0.5750000476837158 Validation: 0.5447999238967896 \n",
      "Loss: Training: 1.2385716438293457 Validation: 1.2913918495178223\n",
      "Epoch 63, CIFAR-10 Batch 5:  Accuracy: Training: 0.550000011920929 Validation: 0.5421999096870422 \n",
      "Loss: Training: 1.190861463546753 Validation: 1.2913180589675903\n",
      "Epoch 64, CIFAR-10 Batch 1:  Accuracy: Training: 0.5249999761581421 Validation: 0.5353999733924866 \n",
      "Loss: Training: 1.3537981510162354 Validation: 1.3103954792022705\n",
      "Epoch 64, CIFAR-10 Batch 2:  Accuracy: Training: 0.5750000476837158 Validation: 0.5473999381065369 \n",
      "Loss: Training: 1.2132138013839722 Validation: 1.3013845682144165\n",
      "Epoch 64, CIFAR-10 Batch 3:  Accuracy: Training: 0.6499999761581421 Validation: 0.5459999442100525 \n",
      "Loss: Training: 1.0493240356445312 Validation: 1.2966790199279785\n",
      "Epoch 64, CIFAR-10 Batch 4:  Accuracy: Training: 0.550000011920929 Validation: 0.5513999462127686 \n",
      "Loss: Training: 1.224306583404541 Validation: 1.2907534837722778\n",
      "Epoch 64, CIFAR-10 Batch 5:  Accuracy: Training: 0.6499999761581421 Validation: 0.5397999286651611 \n",
      "Loss: Training: 1.1798182725906372 Validation: 1.2962242364883423\n",
      "Epoch 65, CIFAR-10 Batch 1:  Accuracy: Training: 0.5249999761581421 Validation: 0.545199990272522 \n",
      "Loss: Training: 1.330706000328064 Validation: 1.314131736755371\n",
      "Epoch 65, CIFAR-10 Batch 2:  Accuracy: Training: 0.550000011920929 Validation: 0.5403999090194702 \n",
      "Loss: Training: 1.2223141193389893 Validation: 1.3058282136917114\n",
      "Epoch 65, CIFAR-10 Batch 3:  Accuracy: Training: 0.625 Validation: 0.5389999151229858 \n",
      "Loss: Training: 1.0586495399475098 Validation: 1.2993404865264893\n",
      "Epoch 65, CIFAR-10 Batch 4:  Accuracy: Training: 0.5750000476837158 Validation: 0.5435999631881714 \n",
      "Loss: Training: 1.2151265144348145 Validation: 1.2957518100738525\n",
      "Epoch 65, CIFAR-10 Batch 5:  Accuracy: Training: 0.5750000476837158 Validation: 0.5309998989105225 \n",
      "Loss: Training: 1.203415870666504 Validation: 1.3049075603485107\n",
      "Epoch 66, CIFAR-10 Batch 1:  Accuracy: Training: 0.6000000238418579 Validation: 0.5463999509811401 \n",
      "Loss: Training: 1.347667932510376 Validation: 1.3065059185028076\n",
      "Epoch 66, CIFAR-10 Batch 2:  Accuracy: Training: 0.5249999761581421 Validation: 0.543999969959259 \n",
      "Loss: Training: 1.186819314956665 Validation: 1.3058521747589111\n",
      "Epoch 66, CIFAR-10 Batch 3:  Accuracy: Training: 0.675000011920929 Validation: 0.5527999401092529 \n",
      "Loss: Training: 1.0467021465301514 Validation: 1.2837257385253906\n",
      "Epoch 66, CIFAR-10 Batch 4:  Accuracy: Training: 0.574999988079071 Validation: 0.5463999509811401 \n",
      "Loss: Training: 1.173166036605835 Validation: 1.2875096797943115\n",
      "Epoch 66, CIFAR-10 Batch 5:  Accuracy: Training: 0.625 Validation: 0.5409999489784241 \n",
      "Loss: Training: 1.1962521076202393 Validation: 1.295596718788147\n",
      "Epoch 67, CIFAR-10 Batch 1:  Accuracy: Training: 0.550000011920929 Validation: 0.5351999402046204 \n",
      "Loss: Training: 1.3244271278381348 Validation: 1.3077141046524048\n",
      "Epoch 67, CIFAR-10 Batch 2:  Accuracy: Training: 0.550000011920929 Validation: 0.542199969291687 \n",
      "Loss: Training: 1.1818327903747559 Validation: 1.2972620725631714\n",
      "Epoch 67, CIFAR-10 Batch 3:  Accuracy: Training: 0.7000000476837158 Validation: 0.5415999293327332 \n",
      "Loss: Training: 1.0574252605438232 Validation: 1.3012148141860962\n",
      "Epoch 67, CIFAR-10 Batch 4:  Accuracy: Training: 0.6000000238418579 Validation: 0.542199969291687 \n",
      "Loss: Training: 1.212299108505249 Validation: 1.2984343767166138\n",
      "Epoch 67, CIFAR-10 Batch 5:  Accuracy: Training: 0.5750000476837158 Validation: 0.5265999436378479 \n",
      "Loss: Training: 1.2254018783569336 Validation: 1.3158745765686035\n",
      "Epoch 68, CIFAR-10 Batch 1:  Accuracy: Training: 0.550000011920929 Validation: 0.5501999855041504 \n",
      "Loss: Training: 1.324671983718872 Validation: 1.2916322946548462\n",
      "Epoch 68, CIFAR-10 Batch 2:  Accuracy: Training: 0.6499999761581421 Validation: 0.5457999110221863 \n",
      "Loss: Training: 1.2026338577270508 Validation: 1.2971400022506714\n",
      "Epoch 68, CIFAR-10 Batch 3:  Accuracy: Training: 0.625 Validation: 0.5427999496459961 \n",
      "Loss: Training: 1.05594801902771 Validation: 1.2810949087142944\n",
      "Epoch 68, CIFAR-10 Batch 4:  Accuracy: Training: 0.550000011920929 Validation: 0.5507999658584595 \n",
      "Loss: Training: 1.269439458847046 Validation: 1.296700358390808\n",
      "Epoch 68, CIFAR-10 Batch 5:  Accuracy: Training: 0.550000011920929 Validation: 0.5225999355316162 \n",
      "Loss: Training: 1.2197598218917847 Validation: 1.3029768466949463\n",
      "Epoch 69, CIFAR-10 Batch 1:  Accuracy: Training: 0.5249999761581421 Validation: 0.5521999597549438 \n",
      "Loss: Training: 1.3368061780929565 Validation: 1.2891755104064941\n",
      "Epoch 69, CIFAR-10 Batch 2:  Accuracy: Training: 0.625 Validation: 0.5345999002456665 \n",
      "Loss: Training: 1.168612003326416 Validation: 1.3072073459625244\n",
      "Epoch 69, CIFAR-10 Batch 3:  Accuracy: Training: 0.625 Validation: 0.5439999103546143 \n",
      "Loss: Training: 1.09938645362854 Validation: 1.2913215160369873\n",
      "Epoch 69, CIFAR-10 Batch 4:  Accuracy: Training: 0.574999988079071 Validation: 0.5501999855041504 \n",
      "Loss: Training: 1.2094156742095947 Validation: 1.2898890972137451\n",
      "Epoch 69, CIFAR-10 Batch 5:  Accuracy: Training: 0.574999988079071 Validation: 0.5379999279975891 \n",
      "Loss: Training: 1.2228803634643555 Validation: 1.3138597011566162\n",
      "Epoch 70, CIFAR-10 Batch 1:  Accuracy: Training: 0.5750000476837158 Validation: 0.5411999225616455 \n",
      "Loss: Training: 1.3453739881515503 Validation: 1.3111014366149902\n",
      "Epoch 70, CIFAR-10 Batch 2:  Accuracy: Training: 0.625 Validation: 0.5409998893737793 \n",
      "Loss: Training: 1.2075988054275513 Validation: 1.3084192276000977\n",
      "Epoch 70, CIFAR-10 Batch 3:  Accuracy: Training: 0.7000000476837158 Validation: 0.5419999957084656 \n",
      "Loss: Training: 1.072393536567688 Validation: 1.299416184425354\n",
      "Epoch 70, CIFAR-10 Batch 4:  Accuracy: Training: 0.5999999642372131 Validation: 0.5459999442100525 \n",
      "Loss: Training: 1.1595546007156372 Validation: 1.276050090789795\n",
      "Epoch 70, CIFAR-10 Batch 5:  Accuracy: Training: 0.6000000238418579 Validation: 0.5407999753952026 \n",
      "Loss: Training: 1.2012155055999756 Validation: 1.282213807106018\n",
      "Epoch 71, CIFAR-10 Batch 1:  Accuracy: Training: 0.5750000476837158 Validation: 0.5361999273300171 \n",
      "Loss: Training: 1.296331524848938 Validation: 1.306960940361023\n",
      "Epoch 71, CIFAR-10 Batch 2:  Accuracy: Training: 0.5750000476837158 Validation: 0.5483999252319336 \n",
      "Loss: Training: 1.1821701526641846 Validation: 1.3037875890731812\n",
      "Epoch 71, CIFAR-10 Batch 3:  Accuracy: Training: 0.7250000238418579 Validation: 0.5433999300003052 \n",
      "Loss: Training: 1.0233488082885742 Validation: 1.2877814769744873\n",
      "Epoch 71, CIFAR-10 Batch 4:  Accuracy: Training: 0.5750000476837158 Validation: 0.5409999489784241 \n",
      "Loss: Training: 1.1724299192428589 Validation: 1.2929580211639404\n",
      "Epoch 71, CIFAR-10 Batch 5:  Accuracy: Training: 0.6000000238418579 Validation: 0.5449999570846558 \n",
      "Loss: Training: 1.1914384365081787 Validation: 1.2729216814041138\n",
      "Epoch 72, CIFAR-10 Batch 1:  Accuracy: Training: 0.550000011920929 Validation: 0.5485999584197998 \n",
      "Loss: Training: 1.2867047786712646 Validation: 1.27485990524292\n",
      "Epoch 72, CIFAR-10 Batch 2:  Accuracy: Training: 0.625 Validation: 0.5509999990463257 \n",
      "Loss: Training: 1.1725980043411255 Validation: 1.2855243682861328\n",
      "Epoch 72, CIFAR-10 Batch 3:  Accuracy: Training: 0.7000000476837158 Validation: 0.5443999171257019 \n",
      "Loss: Training: 1.0364149808883667 Validation: 1.2768950462341309\n",
      "Epoch 72, CIFAR-10 Batch 4:  Accuracy: Training: 0.5750000476837158 Validation: 0.5471999049186707 \n",
      "Loss: Training: 1.136320948600769 Validation: 1.2803411483764648\n",
      "Epoch 72, CIFAR-10 Batch 5:  Accuracy: Training: 0.5750000476837158 Validation: 0.5401999950408936 \n",
      "Loss: Training: 1.184381365776062 Validation: 1.2758136987686157\n",
      "Epoch 73, CIFAR-10 Batch 1:  Accuracy: Training: 0.5249999761581421 Validation: 0.5467999577522278 \n",
      "Loss: Training: 1.2821314334869385 Validation: 1.2920725345611572\n",
      "Epoch 73, CIFAR-10 Batch 2:  Accuracy: Training: 0.625 Validation: 0.5475999116897583 \n",
      "Loss: Training: 1.2052860260009766 Validation: 1.2952640056610107\n",
      "Epoch 73, CIFAR-10 Batch 3:  Accuracy: Training: 0.7000000476837158 Validation: 0.5453999042510986 \n",
      "Loss: Training: 1.0626070499420166 Validation: 1.2867037057876587\n",
      "Epoch 73, CIFAR-10 Batch 4:  Accuracy: Training: 0.5750000476837158 Validation: 0.5539999008178711 \n",
      "Loss: Training: 1.1746845245361328 Validation: 1.2785452604293823\n",
      "Epoch 73, CIFAR-10 Batch 5:  Accuracy: Training: 0.6000000238418579 Validation: 0.5369999408721924 \n",
      "Loss: Training: 1.153635025024414 Validation: 1.2788338661193848\n",
      "Epoch 74, CIFAR-10 Batch 1:  Accuracy: Training: 0.550000011920929 Validation: 0.5539999008178711 \n",
      "Loss: Training: 1.276983618736267 Validation: 1.2853202819824219\n",
      "Epoch 74, CIFAR-10 Batch 2:  Accuracy: Training: 0.5750000476837158 Validation: 0.5317999124526978 \n",
      "Loss: Training: 1.2026541233062744 Validation: 1.306758165359497\n",
      "Epoch 74, CIFAR-10 Batch 3:  Accuracy: Training: 0.7000000476837158 Validation: 0.5453999638557434 \n",
      "Loss: Training: 1.0489736795425415 Validation: 1.282886266708374\n",
      "Epoch 74, CIFAR-10 Batch 4:  Accuracy: Training: 0.5750000476837158 Validation: 0.5577999353408813 \n",
      "Loss: Training: 1.153472661972046 Validation: 1.2682853937149048\n",
      "Epoch 74, CIFAR-10 Batch 5:  Accuracy: Training: 0.6000000238418579 Validation: 0.5431999564170837 \n",
      "Loss: Training: 1.1952717304229736 Validation: 1.286440134048462\n",
      "Epoch 75, CIFAR-10 Batch 1:  Accuracy: Training: 0.5249999761581421 Validation: 0.5485999584197998 \n",
      "Loss: Training: 1.2771588563919067 Validation: 1.2925904989242554\n",
      "Epoch 75, CIFAR-10 Batch 2:  Accuracy: Training: 0.6000000238418579 Validation: 0.545799970626831 \n",
      "Loss: Training: 1.1591315269470215 Validation: 1.2950103282928467\n",
      "Epoch 75, CIFAR-10 Batch 3:  Accuracy: Training: 0.7000000476837158 Validation: 0.550399899482727 \n",
      "Loss: Training: 1.0661890506744385 Validation: 1.28451669216156\n",
      "Epoch 75, CIFAR-10 Batch 4:  Accuracy: Training: 0.625 Validation: 0.5589998960494995 \n",
      "Loss: Training: 1.174673318862915 Validation: 1.2711284160614014\n",
      "Epoch 75, CIFAR-10 Batch 5:  Accuracy: Training: 0.574999988079071 Validation: 0.5403999090194702 \n",
      "Loss: Training: 1.2037391662597656 Validation: 1.2975828647613525\n",
      "Epoch 76, CIFAR-10 Batch 1:  Accuracy: Training: 0.550000011920929 Validation: 0.547999918460846 \n",
      "Loss: Training: 1.2945356369018555 Validation: 1.2988765239715576\n",
      "Epoch 76, CIFAR-10 Batch 2:  Accuracy: Training: 0.6000000238418579 Validation: 0.5461999177932739 \n",
      "Loss: Training: 1.1655341386795044 Validation: 1.2792472839355469\n",
      "Epoch 76, CIFAR-10 Batch 3:  Accuracy: Training: 0.625 Validation: 0.5547999143600464 \n",
      "Loss: Training: 1.052718997001648 Validation: 1.2696704864501953\n",
      "Epoch 76, CIFAR-10 Batch 4:  Accuracy: Training: 0.5750000476837158 Validation: 0.5553999543190002 \n",
      "Loss: Training: 1.1411046981811523 Validation: 1.2704676389694214\n",
      "Epoch 76, CIFAR-10 Batch 5:  Accuracy: Training: 0.6000000238418579 Validation: 0.5501999258995056 \n",
      "Loss: Training: 1.1938449144363403 Validation: 1.2689058780670166\n",
      "Epoch 77, CIFAR-10 Batch 1:  Accuracy: Training: 0.5750000476837158 Validation: 0.5507999062538147 \n",
      "Loss: Training: 1.2685089111328125 Validation: 1.2862530946731567\n",
      "Epoch 77, CIFAR-10 Batch 2:  Accuracy: Training: 0.6000000238418579 Validation: 0.5415999293327332 \n",
      "Loss: Training: 1.1742219924926758 Validation: 1.2952674627304077\n",
      "Epoch 77, CIFAR-10 Batch 3:  Accuracy: Training: 0.6500000357627869 Validation: 0.5555998682975769 \n",
      "Loss: Training: 1.0612412691116333 Validation: 1.2749823331832886\n",
      "Epoch 77, CIFAR-10 Batch 4:  Accuracy: Training: 0.550000011920929 Validation: 0.5459999442100525 \n",
      "Loss: Training: 1.1577600240707397 Validation: 1.2784940004348755\n",
      "Epoch 77, CIFAR-10 Batch 5:  Accuracy: Training: 0.6000000238418579 Validation: 0.547999918460846 \n",
      "Loss: Training: 1.1832075119018555 Validation: 1.269429087638855\n",
      "Epoch 78, CIFAR-10 Batch 1:  Accuracy: Training: 0.550000011920929 Validation: 0.5467999577522278 \n",
      "Loss: Training: 1.3166382312774658 Validation: 1.2957689762115479\n",
      "Epoch 78, CIFAR-10 Batch 2:  Accuracy: Training: 0.6000000238418579 Validation: 0.5443999767303467 \n",
      "Loss: Training: 1.138384222984314 Validation: 1.2918355464935303\n",
      "Epoch 78, CIFAR-10 Batch 3:  Accuracy: Training: 0.6499999761581421 Validation: 0.5435999631881714 \n",
      "Loss: Training: 1.0543020963668823 Validation: 1.2759381532669067\n",
      "Epoch 78, CIFAR-10 Batch 4:  Accuracy: Training: 0.625 Validation: 0.545199990272522 \n",
      "Loss: Training: 1.1164965629577637 Validation: 1.2876474857330322\n",
      "Epoch 78, CIFAR-10 Batch 5:  Accuracy: Training: 0.6000000238418579 Validation: 0.5413999557495117 \n",
      "Loss: Training: 1.2173779010772705 Validation: 1.2925801277160645\n",
      "Epoch 79, CIFAR-10 Batch 1:  Accuracy: Training: 0.550000011920929 Validation: 0.5521999597549438 \n",
      "Loss: Training: 1.2732254266738892 Validation: 1.2873098850250244\n",
      "Epoch 79, CIFAR-10 Batch 2:  Accuracy: Training: 0.625 Validation: 0.5485999584197998 \n",
      "Loss: Training: 1.1601616144180298 Validation: 1.2894420623779297\n",
      "Epoch 79, CIFAR-10 Batch 3:  Accuracy: Training: 0.7250000238418579 Validation: 0.5351999998092651 \n",
      "Loss: Training: 1.0048093795776367 Validation: 1.294467568397522\n",
      "Epoch 79, CIFAR-10 Batch 4:  Accuracy: Training: 0.6000000238418579 Validation: 0.5473998785018921 \n",
      "Loss: Training: 1.1220394372940063 Validation: 1.2778925895690918\n",
      "Epoch 79, CIFAR-10 Batch 5:  Accuracy: Training: 0.5750000476837158 Validation: 0.5383999943733215 \n",
      "Loss: Training: 1.204137921333313 Validation: 1.2843674421310425\n",
      "Epoch 80, CIFAR-10 Batch 1:  Accuracy: Training: 0.550000011920929 Validation: 0.5539999604225159 \n",
      "Loss: Training: 1.2608811855316162 Validation: 1.2741270065307617\n",
      "Epoch 80, CIFAR-10 Batch 2:  Accuracy: Training: 0.6000000238418579 Validation: 0.5517999529838562 \n",
      "Loss: Training: 1.1664959192276 Validation: 1.290022611618042\n",
      "Epoch 80, CIFAR-10 Batch 3:  Accuracy: Training: 0.6499999761581421 Validation: 0.5443999171257019 \n",
      "Loss: Training: 1.043930172920227 Validation: 1.2828483581542969\n",
      "Epoch 80, CIFAR-10 Batch 4:  Accuracy: Training: 0.6000000238418579 Validation: 0.5573999285697937 \n",
      "Loss: Training: 1.1302216053009033 Validation: 1.2672017812728882\n",
      "Epoch 80, CIFAR-10 Batch 5:  Accuracy: Training: 0.550000011920929 Validation: 0.5401999354362488 \n",
      "Loss: Training: 1.1918894052505493 Validation: 1.2731096744537354\n",
      "Epoch 81, CIFAR-10 Batch 1:  Accuracy: Training: 0.550000011920929 Validation: 0.5489999055862427 \n",
      "Loss: Training: 1.280641794204712 Validation: 1.2814648151397705\n",
      "Epoch 81, CIFAR-10 Batch 2:  Accuracy: Training: 0.625 Validation: 0.549799919128418 \n",
      "Loss: Training: 1.1753453016281128 Validation: 1.2789934873580933\n",
      "Epoch 81, CIFAR-10 Batch 3:  Accuracy: Training: 0.7250000238418579 Validation: 0.5443999171257019 \n",
      "Loss: Training: 1.0266673564910889 Validation: 1.271907091140747\n",
      "Epoch 81, CIFAR-10 Batch 4:  Accuracy: Training: 0.6000000238418579 Validation: 0.5545998811721802 \n",
      "Loss: Training: 1.1301732063293457 Validation: 1.2718034982681274\n",
      "Epoch 81, CIFAR-10 Batch 5:  Accuracy: Training: 0.6000000238418579 Validation: 0.5427999496459961 \n",
      "Loss: Training: 1.1919713020324707 Validation: 1.2838634252548218\n",
      "Epoch 82, CIFAR-10 Batch 1:  Accuracy: Training: 0.550000011920929 Validation: 0.5585999488830566 \n",
      "Loss: Training: 1.2738149166107178 Validation: 1.2805371284484863\n",
      "Epoch 82, CIFAR-10 Batch 2:  Accuracy: Training: 0.6000000238418579 Validation: 0.556399941444397 \n",
      "Loss: Training: 1.158941626548767 Validation: 1.2735093832015991\n",
      "Epoch 82, CIFAR-10 Batch 3:  Accuracy: Training: 0.7749999761581421 Validation: 0.5415999293327332 \n",
      "Loss: Training: 1.0066269636154175 Validation: 1.2767082452774048\n",
      "Epoch 82, CIFAR-10 Batch 4:  Accuracy: Training: 0.6499999761581421 Validation: 0.5493999719619751 \n",
      "Loss: Training: 1.1399224996566772 Validation: 1.2771599292755127\n",
      "Epoch 82, CIFAR-10 Batch 5:  Accuracy: Training: 0.550000011920929 Validation: 0.542199969291687 \n",
      "Loss: Training: 1.2027772665023804 Validation: 1.2822867631912231\n",
      "Epoch 83, CIFAR-10 Batch 1:  Accuracy: Training: 0.6000000238418579 Validation: 0.5445999503135681 \n",
      "Loss: Training: 1.245056390762329 Validation: 1.280120611190796\n",
      "Epoch 83, CIFAR-10 Batch 2:  Accuracy: Training: 0.625 Validation: 0.5453999042510986 \n",
      "Loss: Training: 1.1839475631713867 Validation: 1.2907509803771973\n",
      "Epoch 83, CIFAR-10 Batch 3:  Accuracy: Training: 0.7000000476837158 Validation: 0.5427999496459961 \n",
      "Loss: Training: 1.001291275024414 Validation: 1.2750523090362549\n",
      "Epoch 83, CIFAR-10 Batch 4:  Accuracy: Training: 0.6250000596046448 Validation: 0.5473999977111816 \n",
      "Loss: Training: 1.117722988128662 Validation: 1.269852638244629\n",
      "Epoch 83, CIFAR-10 Batch 5:  Accuracy: Training: 0.5999999642372131 Validation: 0.5445999503135681 \n",
      "Loss: Training: 1.1790388822555542 Validation: 1.2631884813308716\n",
      "Epoch 84, CIFAR-10 Batch 1:  Accuracy: Training: 0.550000011920929 Validation: 0.5509999394416809 \n",
      "Loss: Training: 1.2918716669082642 Validation: 1.2890877723693848\n",
      "Epoch 84, CIFAR-10 Batch 2:  Accuracy: Training: 0.5750000476837158 Validation: 0.5529999732971191 \n",
      "Loss: Training: 1.1498304605484009 Validation: 1.2775678634643555\n",
      "Epoch 84, CIFAR-10 Batch 3:  Accuracy: Training: 0.75 Validation: 0.549799919128418 \n",
      "Loss: Training: 1.0231842994689941 Validation: 1.2713379859924316\n",
      "Epoch 84, CIFAR-10 Batch 4:  Accuracy: Training: 0.625 Validation: 0.556399941444397 \n",
      "Loss: Training: 1.1112308502197266 Validation: 1.2791653871536255\n",
      "Epoch 84, CIFAR-10 Batch 5:  Accuracy: Training: 0.6250000596046448 Validation: 0.5395999550819397 \n",
      "Loss: Training: 1.2149536609649658 Validation: 1.2871274948120117\n",
      "Epoch 85, CIFAR-10 Batch 1:  Accuracy: Training: 0.6000000238418579 Validation: 0.5487999320030212 \n",
      "Loss: Training: 1.25310480594635 Validation: 1.28996741771698\n",
      "Epoch 85, CIFAR-10 Batch 2:  Accuracy: Training: 0.6000000238418579 Validation: 0.5501999258995056 \n",
      "Loss: Training: 1.117757797241211 Validation: 1.2827095985412598\n",
      "Epoch 85, CIFAR-10 Batch 3:  Accuracy: Training: 0.7000000476837158 Validation: 0.5497999787330627 \n",
      "Loss: Training: 1.016947865486145 Validation: 1.2740542888641357\n",
      "Epoch 85, CIFAR-10 Batch 4:  Accuracy: Training: 0.6000000238418579 Validation: 0.5513999462127686 \n",
      "Loss: Training: 1.1373097896575928 Validation: 1.2708075046539307\n",
      "Epoch 85, CIFAR-10 Batch 5:  Accuracy: Training: 0.574999988079071 Validation: 0.5377999544143677 \n",
      "Loss: Training: 1.2165721654891968 Validation: 1.2788442373275757\n",
      "Epoch 86, CIFAR-10 Batch 1:  Accuracy: Training: 0.5750000476837158 Validation: 0.5487998723983765 \n",
      "Loss: Training: 1.2369639873504639 Validation: 1.2749669551849365\n",
      "Epoch 86, CIFAR-10 Batch 2:  Accuracy: Training: 0.6000000238418579 Validation: 0.5483999252319336 \n",
      "Loss: Training: 1.1104755401611328 Validation: 1.282631754875183\n",
      "Epoch 86, CIFAR-10 Batch 3:  Accuracy: Training: 0.7000000476837158 Validation: 0.5461999177932739 \n",
      "Loss: Training: 0.9975206255912781 Validation: 1.2703887224197388\n",
      "Epoch 86, CIFAR-10 Batch 4:  Accuracy: Training: 0.5750000476837158 Validation: 0.5571999549865723 \n",
      "Loss: Training: 1.137560486793518 Validation: 1.2613654136657715\n",
      "Epoch 86, CIFAR-10 Batch 5:  Accuracy: Training: 0.6000000238418579 Validation: 0.5491999387741089 \n",
      "Loss: Training: 1.1840405464172363 Validation: 1.2634302377700806\n",
      "Epoch 87, CIFAR-10 Batch 1:  Accuracy: Training: 0.5750000476837158 Validation: 0.5561999082565308 \n",
      "Loss: Training: 1.2551848888397217 Validation: 1.2674363851547241\n",
      "Epoch 87, CIFAR-10 Batch 2:  Accuracy: Training: 0.625 Validation: 0.5549999475479126 \n",
      "Loss: Training: 1.123099684715271 Validation: 1.266781210899353\n",
      "Epoch 87, CIFAR-10 Batch 3:  Accuracy: Training: 0.7250000238418579 Validation: 0.5463999509811401 \n",
      "Loss: Training: 1.0082292556762695 Validation: 1.2660934925079346\n",
      "Epoch 87, CIFAR-10 Batch 4:  Accuracy: Training: 0.625 Validation: 0.5493999123573303 \n",
      "Loss: Training: 1.1105015277862549 Validation: 1.2611688375473022\n",
      "Epoch 87, CIFAR-10 Batch 5:  Accuracy: Training: 0.6000000238418579 Validation: 0.5465999245643616 \n",
      "Loss: Training: 1.200954556465149 Validation: 1.2767624855041504\n",
      "Epoch 88, CIFAR-10 Batch 1:  Accuracy: Training: 0.6000000238418579 Validation: 0.5473999381065369 \n",
      "Loss: Training: 1.248532772064209 Validation: 1.297071933746338\n",
      "Epoch 88, CIFAR-10 Batch 2:  Accuracy: Training: 0.625 Validation: 0.5447999238967896 \n",
      "Loss: Training: 1.1123507022857666 Validation: 1.294264316558838\n",
      "Epoch 88, CIFAR-10 Batch 3:  Accuracy: Training: 0.75 Validation: 0.5475999116897583 \n",
      "Loss: Training: 1.0289177894592285 Validation: 1.268955111503601\n",
      "Epoch 88, CIFAR-10 Batch 4:  Accuracy: Training: 0.6000000238418579 Validation: 0.5525999069213867 \n",
      "Loss: Training: 1.1313555240631104 Validation: 1.2646913528442383\n",
      "Epoch 88, CIFAR-10 Batch 5:  Accuracy: Training: 0.625 Validation: 0.5511999130249023 \n",
      "Loss: Training: 1.187716007232666 Validation: 1.263241171836853\n",
      "Epoch 89, CIFAR-10 Batch 1:  Accuracy: Training: 0.550000011920929 Validation: 0.5531999468803406 \n",
      "Loss: Training: 1.2654783725738525 Validation: 1.277858018875122\n",
      "Epoch 89, CIFAR-10 Batch 2:  Accuracy: Training: 0.625 Validation: 0.5517999529838562 \n",
      "Loss: Training: 1.1118879318237305 Validation: 1.2921600341796875\n",
      "Epoch 89, CIFAR-10 Batch 3:  Accuracy: Training: 0.7250000238418579 Validation: 0.5546000003814697 \n",
      "Loss: Training: 0.9850873947143555 Validation: 1.2553257942199707\n",
      "Epoch 89, CIFAR-10 Batch 4:  Accuracy: Training: 0.5750000476837158 Validation: 0.5507999062538147 \n",
      "Loss: Training: 1.1327335834503174 Validation: 1.2612013816833496\n",
      "Epoch 89, CIFAR-10 Batch 5:  Accuracy: Training: 0.6000000238418579 Validation: 0.5511999130249023 \n",
      "Loss: Training: 1.163757562637329 Validation: 1.2635711431503296\n",
      "Epoch 90, CIFAR-10 Batch 1:  Accuracy: Training: 0.5750000476837158 Validation: 0.553399920463562 \n",
      "Loss: Training: 1.2471816539764404 Validation: 1.261732816696167\n",
      "Epoch 90, CIFAR-10 Batch 2:  Accuracy: Training: 0.625 Validation: 0.5509999394416809 \n",
      "Loss: Training: 1.1270091533660889 Validation: 1.2734596729278564\n",
      "Epoch 90, CIFAR-10 Batch 3:  Accuracy: Training: 0.7750000357627869 Validation: 0.5485999584197998 \n",
      "Loss: Training: 1.0078012943267822 Validation: 1.2636711597442627\n",
      "Epoch 90, CIFAR-10 Batch 4:  Accuracy: Training: 0.6500000357627869 Validation: 0.549799919128418 \n",
      "Loss: Training: 1.1010055541992188 Validation: 1.277056097984314\n",
      "Epoch 90, CIFAR-10 Batch 5:  Accuracy: Training: 0.6500000357627869 Validation: 0.5433999300003052 \n",
      "Loss: Training: 1.1506078243255615 Validation: 1.2666078805923462\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.5430181962025317\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEZFIg2MRERERkej/Ay/zl7+tmSVaAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f76369594a8>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Why 50-70% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 70%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
